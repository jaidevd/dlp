{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ebb63-6b68-40e4-a3e1-023eaa0862fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the HF cache before starting\n",
    "!rm -rf /home/jaidevd/.cache/huggingface/datasets/ai4bharat___indic_glue/\n",
    "!rm -rf /home/jaidevd/.cache/huggingface/datasets//home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae5ea9-2ef2-43bb-84cd-10aed8f6c3fc",
   "metadata": {},
   "source": [
    "1. Choose all the correct statements about the dataset at the following URL: https://huggingface.co/datasets/ai4bharat/naamapadam\n",
    "\n",
    "Note: We strongly recommend using the appropriate functions in the datasets module to obtain the dataset’s information, rather than access\u0002ing that information directly from the hub.\n",
    "\n",
    "- [ ] The dataset contains 11 configs(subsets)\n",
    "- [ ] The dataset contains 22 configs(subsets)\n",
    "- [ ] The Hindi subset of the dataset contains 985787 training samples\n",
    "- [ ] The Tamil subset of the dataset contains a total of 985787 samples.\n",
    "- [ ] The number of classes (labels,tags) is 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f1b1ee-e4e2-44eb-b4ae-c6de53cf83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863bfac7-9d72-4298-bd3f-0788620055b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'bn', 'gu', 'hi', 'kn', 'ml', 'mr', 'or', 'pa', 'ta', 'te']\n"
     ]
    }
   ],
   "source": [
    "# How many configs?\n",
    "cfg_names = datasets.get_dataset_config_names('ai4bharat/naamapadam')\n",
    "print(cfg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe439bc-a0ea-42e5-aa50-4fc1619877b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cfg_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378ffed2-efda-4685-82a3-ccb84cc7b6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772af29a3f984e1c8b4fa02d91ef2388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad884c78f2ca48818a8069fa82076918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fea49642561444aac526196ecbed868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c65e82616f4f95b4e9c50c766fc72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1b5662fbbf4ffaaad9593248738f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/985787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d7214f27ef4e3b920162ff8970abb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1153256a38a54e5b9b2e9c424ce9658c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 985787\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# The Hindi subset of the dataset contains 985787 training samples\n",
    "hi = datasets.load_dataset(\"ai4bharat/naamapadam\", name=\"hi\", split=\"train\")\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec70f70b-336a-4bcf-acf4-a19f1b13b5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985787"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f65184f-d4b6-46ca-ac70-78b119acb929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5023e6d1b834a7cb0a4dc360e841b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/48.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce133029d5624e5d8fbe7e95bc025780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/85.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133d71ca870c4e0eb3358f077573b1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/275k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cbd4057749412d956dd73d8cc0c48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/497882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584a2210c5124c5db468e229eac92d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/758 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c1246653ac4deca8df56e3bcf84b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 497882\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 758\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 2795\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# The Tamil subset of the dataset contains a total of 985787 samples.\n",
    "ta = datasets.load_dataset(\"ai4bharat/naamapadam\", name=\"ta\")\n",
    "print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c41c5ccc-f751-4efd-9201-a63d1911419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501435\n"
     ]
    }
   ],
   "source": [
    "n_samples = 0\n",
    "for k, v in ta.items():\n",
    "    n_samples += len(v)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c38fb1a-734c-4230-9cf8-1f3b8b1b7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "# The number of classes (labels,tags) is 7\n",
    "tags = set()\n",
    "for sample in ta['train']['ner_tags']:\n",
    "    tags.update(sample)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb27ef4-bafc-434f-8f84-8e1f1e76274b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e10eb-3def-46aa-906d-a72201585031",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074467d-3c80-40e2-8fc3-8f2dcd06c012",
   "metadata": {},
   "source": [
    "2. Download the Tamil sub-dataset from the following URL https://huggingface. co/datasets/ai4bharat/naamapadam, and store in a variable named “ds”.\n",
    "\n",
    "Find the location of the cache directory (one way to determine the cache directory location is by using a method from the Dataset class; please refer to the documentation). Which of the following files are in the cached directory corresponding to the downloaded dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e6d5217-a380-4805-a5d9-964971d8d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directories:\n",
      "/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n",
      "/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n",
      "/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n",
      "\n",
      "\n",
      "Content of cache directory:\n",
      "['dataset_info.json', 'naamapadam-validation.arrow', 'naamapadam-test.arrow', 'LICENSE', 'naamapadam-train.arrow']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ds = datasets.load_dataset(\"ai4bharat/naamapadam\", name=\"ta\")\n",
    "print('Cache directories:')\n",
    "for _, files in ds.cache_files.items():\n",
    "    for file in files:\n",
    "        cache_dir = os.path.dirname(file['filename'])\n",
    "        print(cache_dir)\n",
    "\n",
    "print(\"\\n\\nContent of cache directory:\")\n",
    "print(os.listdir(cache_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f651098-3d26-483f-afcc-a41aa63a9750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_info.json',\n",
       " 'naamapadam-validation.arrow',\n",
       " 'naamapadam-test.arrow',\n",
       " 'LICENSE',\n",
       " 'naamapadam-train.arrow']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files in the cache directory\n",
    "os.listdir(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9080667-8f8d-444a-934b-478789795a0c",
   "metadata": {},
   "source": [
    "---\n",
    "3. What is the memory size of the downloaded dataset in MB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd54055-4939-45c2-84ec-d72600e8876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180M\t/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!du -h --max-depth 1 /home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfdaa6d-5fbe-4011-9f84-cddc52802581",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c01bd-f092-4abd-9fa2-b5c3f55f8dba",
   "metadata": {},
   "source": [
    "4. What is the number of training samples in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a9916b-bd21-47e6-b37d-7494aef93112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 497882\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 758\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 2795\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "510e3cc8-6643-4d97-a833-c3fb84634183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497882"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b34b1-7359-44e7-8e90-4687d102d564",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dcc06-0f31-4156-9dba-4a5f2e5d11c7",
   "metadata": {},
   "source": [
    "5. Create a new column named “num tokens”. Compute the number of tokens (words) in each sample and store the results in the newly created column. Reassign the modified dataset to the same variable “ds”. How many tokens (in millions) are there in the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "209cde8f-1d00-4392-9dbb-5ea70c3e07ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b710e4df84bcc9503bd66f78a118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/497882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fbdd1869bb4cbe8ce26f653b12d7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/758 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eb1599472d4e5ea436c0ab6105de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2795 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'num_tokens'],\n",
       "        num_rows: 497882\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'num_tokens'],\n",
       "        num_rows: 758\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'num_tokens'],\n",
       "        num_rows: 2795\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = ds.map(lambda x: {'num_tokens': len(x['tokens'])})\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29ba9ea8-c71f-4586-8169-14ade984db94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.001876\n"
     ]
    }
   ],
   "source": [
    "n_tokens = 0\n",
    "for k, v in num_tokens.items():\n",
    "    for sample in v['num_tokens']:\n",
    "        n_tokens += sample\n",
    "print(n_tokens / 1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81403de8-85a4-4c28-a266-e20ae5b32ec4",
   "metadata": {},
   "source": [
    "---\n",
    "6. The statement that the modified dataset increased the memory requirement about two fold is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "851f191d-23d4-4b3b-9884-a2bd01561470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363M\t/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n"
     ]
    }
   ],
   "source": [
    "!du -h --max-depth=1 /home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487d227-24cd-4e9e-b84c-ec9419c88196",
   "metadata": {},
   "source": [
    "---\n",
    "7.  Concatenate all the samples across the splits in the following order: [train:test:validation]. Currently, each sample contains a list of words (tokens). Create a sentence by joining the individual words (tokens) in each sample using a single white space as a delimiter and store the re\u0002sulting sample in a new column named “text”. Create a new dataset by removing the columns “ner tags” and “tokens”. Store the new dataset in the same variable “ds”. Enter the total number of samples in “ds”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dcbbb91-4e0c-4799-98f0-499f3aa0a363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30cc87312a34e11a3cdcad0ae9bd486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/501435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = ['train', 'test', 'validation']\n",
    "ta_concatenated = datasets.concatenate_datasets([ds[spl] for spl in splits])\n",
    "text = ta_concatenated.map(lambda x: {'text': ' '.join(x['tokens'])})\n",
    "ds = text.remove_columns(['ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "309ea7af-6c3e-44fd-a3b3-441e9b3fe1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 501435\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3b982-f2a8-4ef8-8465-1ef2b84590f0",
   "metadata": {},
   "source": [
    "---\n",
    "8. Each modification to the dataset introduces new cache file in the cache directory, thereby increasing the disk space. The statement is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2ea8ad-6203-4655-9118-2edb2983b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 671292\n",
      "-rw-rw-r-- 1 jaidevd jaidevd 307337744 Oct  5 20:07 cache-2f2a0680cbdd4aa2.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd   1070552 Oct  5 20:01 cache-99fff4b89572c4ac.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd    326920 Oct  5 20:01 cache-110b811898aab4aa.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd 190636448 Oct  5 20:01 cache-606119b96cd22d43.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd      1377 Oct  5 19:44 dataset_info.json\n",
      "-rw-rw-r-- 1 jaidevd jaidevd        75 Oct  5 19:44 LICENSE\n",
      "-rw-rw-r-- 1 jaidevd jaidevd   1047912 Oct  5 19:44 naamapadam-validation.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd    320672 Oct  5 19:44 naamapadam-test.arrow\n",
      "-rw-rw-r-- 1 jaidevd jaidevd 186629352 Oct  5 19:44 naamapadam-train.arrow\n"
     ]
    }
   ],
   "source": [
    "!ls -lt {cache_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80229128-5c95-43bd-91ca-32817d62091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656M\t/home/jaidevd/.cache/huggingface/datasets/ai4bharat___naamapadam/ta/1.0.0/9d4f21ac57d11ed4f9ea64854fdc9f5618e61acc\n"
     ]
    }
   ],
   "source": [
    "!du -h --max-depth=1 {cache_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c951d-73b4-42fd-a93e-c7ed15419008",
   "metadata": {},
   "source": [
    "---\n",
    "9. Filter the dataset so that all the samples in the dataset should have at least six tokens (any symbol separated by a white space is considered a token). Enter the number of samples in the dataset after filtering. Enter the exact number. If the answer is 123456, enter it as 123456."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1fd42e3-987e-4973-8418-e7213b8ae070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb091485f694d789ef30870bf17d916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/501435 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.filter(lambda x: len(x['text'].split(' ')) >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0654b3ba-720e-49d3-aab0-255857fc19ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 370495\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae862abe-02e5-4b87-bc0f-9f12a0d30edd",
   "metadata": {},
   "source": [
    "---\n",
    "10. Download all the splits of Tamil sub-dataset “inltkh.ta” of https://huggingface.co/datasets/ai4bharat/indic_glue .\n",
    "\n",
    "Filter the dataset so that each sample contains at least six words (separated by a single white space).Then Interleave the resultant dataset with the above filtered dataset of naamapadam. Take 80% of samples from naamapadam and 20% from indic glue. Enter the number of samples after interleaving the datasets.\n",
    "Note: Set the value of the seed argument to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c555ef43-1a76-4537-ae27-ffafe19af791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 6684\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue = datasets.load_dataset('ai4bharat/indic_glue', name=\"inltkh.ta\", split='all')\n",
    "glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39b09944-95ad-4f5f-ad7d-16900ac5c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = glue.filter(lambda x: len(x['text'].split(' ')) >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "396ac8b1-f3c9-4642-a335-19ad6c3accb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 6428\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "548d2779-936f-4faa-ac4e-9f46cb03ed87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 370495\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c8a2bd1-7bd6-4985-8365-7cbc729a3b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 32354\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.interleave_datasets([ds, glue], probabilities=[0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fda9e5-298a-4de1-a011-d2c8cbe539c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
