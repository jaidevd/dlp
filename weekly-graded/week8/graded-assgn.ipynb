{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d3996b-dfbd-44c3-91e3-1c482b13f9c7",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Dataset\n",
    "* Use the “VoxPopuli” dataset available on Hugging Face, specifically the “it” (Italian) subset, and load only the training split.\n",
    "* Shuffle the dataset and take a random quarter (with seed=42) of the entries. This smaller subset will reduce processing time, making it easier to handle on limited resources.\n",
    "* Convert Audio Sampling Rate: Convert the audio samples in the dataset to a 16 kHz sampling rate, as this is compatible with the model you’ll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15635a12-3dfb-4e6c-95ad-874ca30bcf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdf65acdd54477a86a2b615e55eb0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "asr_train.tsv:   0%|          | 0.00/10.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acef2fd6d0c410580799002bdefb0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "asr_dev.tsv:   0%|          | 0.00/602k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b752d6dd698043c0abcaee0bf3de9e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "asr_test.tsv:   0%|          | 0.00/573k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd265e1619a14eba97b0f4a55e4e889d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_part_0.tar.gz:   0%|          | 0.00/2.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e78e59a4847e4bc8207c052d0f4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_part_1.tar.gz:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936f9c91503b403db09e2ad58aea57ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_part_2.tar.gz:   0%|          | 0.00/2.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1279a1dacba141d39471ea25a3d6e345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_part_3.tar.gz:   0%|          | 0.00/2.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c2d00dcd414741856a068fc083d063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_part_4.tar.gz:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf00990765b4007acb753e529b7cf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev_part_0.tar.gz:   0%|          | 0.00/565M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d5bf38c1a24aaf9bca849055cb46e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_part_0.tar.gz:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d702dbc0b541e2957f7a0cabdcc3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326f121ec5be4b708705a58b0cb3d75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfa22765bae46db9e7da680f6232e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('facebook/voxpopuli', 'it', split='train')\n",
    "ds = ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968d331-8592-4fb3-9c81-8dedcb3db5f4",
   "metadata": {},
   "source": [
    "1. What is the original size of the train split of ”facebook/voxpopuli”, ”train” set for ”italian” ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e50565-d1ad-4f0e-9f39-36fcf551e4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_id', 'language', 'audio', 'raw_text', 'normalized_text', 'gender', 'speaker_id', 'is_gold_transcript', 'accent'],\n",
       "    num_rows: 22576\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7ec9d-0a14-4885-a241-8c0705470611",
   "metadata": {},
   "source": [
    "2. What is the sampling rate of the original audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7e59ec-64eb-4d0a-ac05-9ce4c224f9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726984f6c0154b5caf727b9d213873db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{16000}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ds.map(lambda x: {'sr': x['audio']['sampling_rate']}, remove_columns=ds.column_names)['sr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572de0f-501c-4400-b531-df938c57567e",
   "metadata": {},
   "source": [
    "3. How many unique characters are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c73271f-3c2c-4d27-bb7d-648910dc70a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_chars = set()\n",
    "for sample in ds:\n",
    "    uniq_chars.update(sample['normalized_text'])\n",
    "len(uniq_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6360f274-01a4-4cf1-8d20-980b0a98e7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_chars = set()\n",
    "for sample in ds:\n",
    "    uniq_chars.update(sample['raw_text'])\n",
    "len(uniq_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fb554-207f-4011-962a-06e30b480d95",
   "metadata": {},
   "source": [
    "4. How many tokens are in the ”microsoft/speechT5” tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26dfc295-f6a5-4855-8012-fa3ba56bf8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Tokenizer\n",
    "tokenizer = SpeechT5Tokenizer.from_pretrained('microsoft/speecht5_tts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c69cd7-8e0e-40e1-8162-1159952fad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b357387-49d2-44d3-bc5a-70535cf9ca2a",
   "metadata": {},
   "source": [
    "5. Whether all the unique characters in the italian train split are present in the token list of mi crosoft/speechT5? (true\\false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d7d740-f3e0-462f-9d31-ffe9e7057c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_chars = set()\n",
    "for sample in ds:\n",
    "    uniq_chars.update(sample['normalized_text'])\n",
    "len(uniq_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa60104-9488-4927-a18e-5ab84e9bf88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', 'à', 'è', 'ì', 'í', 'ï', 'ò', 'ó', 'ù'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_chars - vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79087713-b92d-441a-9e84-56e1bea3f6e6",
   "metadata": {},
   "source": [
    "7. How many speakers have less than or equal to 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d39c085-f367-4bb1-be1a-8addd072f139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124835     519\n",
       "124851    1350\n",
       "124778     236\n",
       "96818      190\n",
       "None      1248\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "speaker_ids = Counter(ds['speaker_id'])\n",
    "speaker_ids = pd.Series(speaker_ids)\n",
    "speaker_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f89463e-f0ef-44cd-8167-771ee0bb7e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(speaker_ids <= 100).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75a995-f285-470e-8bb2-2c5186a17a6c",
   "metadata": {},
   "source": [
    "8. What is the length of the dataset after removing speakers with less than 100 samples and more than 400 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b93a61-7399-4176-aea6-bdda8a98f6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556fb22ae18044a1a1245d9941edefc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/22576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio_id', 'language', 'audio', 'raw_text', 'normalized_text', 'gender', 'speaker_id', 'is_gold_transcript', 'accent'],\n",
       "    num_rows: 10683\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_by_length(sample):\n",
    "    n_samples = speaker_ids[sample['speaker_id']]\n",
    "    return (n_samples >= 100) and (n_samples <= 400)\n",
    "ds_moderate = ds.filter(filter_by_length)\n",
    "ds_moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41026f71-9a0e-4543-9d5d-16154155fc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
