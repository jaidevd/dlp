{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c2be0d-3a69-4f38-90a7-e4f3891ffc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2449d5a0acef4905b5eb0a1c887af8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ebcd130ca948ba8a9655fd2e85528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42d997b799b458f8c43c0efdec1923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454d7c3e0ac04c22bfe1ef6ee60473fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfa858787b546f7908cc2fbad8def3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c637b29c14584f5fb6a98839bce772e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e67627a9f44e73b9e931361c0330b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset('stanfordnlp/imdb')\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf341de-5012-41ea-811d-b0f9d3e1edc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_train_split = imdb_dataset['train']\n",
    "print(imdb_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb172611-2b19-4217-8731-cfce9d0260a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = imdb_dataset.pop('unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7648acb1-ae8e-40bf-ba9f-1dd9a66c8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4310cafc-13f8-4825-8130-b91c5b239d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_split = load_dataset('stanfordnlp/imdb', split=\"train\")\n",
    "print(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c2aa06-409e-429e-a26e-34ebd27d1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = train_split.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf015a7d-e995-48ae-b7fa-2de93d52957f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(small_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d8474-48c0-4423-bdad-6f06f6732667",
   "metadata": {},
   "source": [
    "### Accessing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa3dd1b-8b66-45cc-bab0-ba6117aa9a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'Although I have to admit I laughed more watching this movie than the '\n",
      "         'last few comedies I saw.<br /><br />The budget must have consisted '\n",
      "         'of pocket change from the actors. The production values are so low '\n",
      "         'that they actual made it kind of fun to watch. Reminds me of the '\n",
      "         'Robot Monster made up of a guy in a gorilla suit with a cardboard '\n",
      "         'diving helmet on.<br /><br />In one scene a hapless victim gets '\n",
      "         'their arm and leg cut off. Geez, hard to believe but the Black '\n",
      "         'Knight scene from Holy Grail was more realistic. I kept wondering '\n",
      "         'why the victim didn\\'t start shouting \" None Shall Pass\" and \" It\\'s '\n",
      "         'only a flesh wound, I\\'ve had worse\". It was one of the funniest '\n",
      "         'scenes I\\'ve seen in the past year.<br /><br />The \"gladiator/demon\" '\n",
      "         'was a stitch too. Between the horribly cheap costume and the geeky '\n",
      "         'look of the guy in it the end result was hysterical.<br /><br '\n",
      "         '/>Truly a movie that is bad enough to be watchable. Kind of like '\n",
      "         'seeing a slow motion auto accident on film.<br /><br />'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "idx = 1000\n",
    "example = imdb_dataset['train'][idx]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f775e2-f2ae-4464-acdb-e1943eeabe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "example = imdb_dataset['train'].select([idx])\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c17ff4d0-7861-4054-b72e-a91e5ab03836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "idx = range(0, 100, 2)\n",
    "examples = imdb_dataset['train'].select(idx)\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9442f327-7791-432e-9afa-a96f718281a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46907083103a4fcca44e3451d89b96b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en']\n",
      "['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "# translation dataset - WMT-14\n",
    "\n",
    "from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "print(get_dataset_config_names('wmt/wmt14'))\n",
    "print(get_dataset_split_names('wmt/wmt14', 'hi-en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7ca1ee-db3c-41aa-8790-6186d937f7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6cce3981f844d58e4de6838882b343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/992k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2281a891253f4814a15927698872663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/85.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4223a12fce4a0288a7194078dce4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/506k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0f3a1a5471442486974ddf02454652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/32863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0768532a6c8c4af490a340242e0ffcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed14b2b5beaf4e40b87fd1e246b8ef94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 32863\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "translation_dataset = load_dataset(path=\"wmt/wmt14\", name=\"hi-en\")\n",
    "print(translation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d70ad1-4511-442b-803f-6e2883b3bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 35890\n",
      "})\n",
      "35890\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(path=\"wmt/wmt14\", name=\"hi-en\", split=\"train+test+validation\")\n",
    "print(raw_dataset)\n",
    "print(len(raw_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314a8d01-824f-4cc1-946a-7123e7cde2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': Translation(languages=['hi', 'en'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(translation_dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288ace24-081a-45d6-ae78-6765068ab0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2111f60ba05d48dd91ee40ac6dcc5be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1634ffaf66b34a0a846ad26a76966af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad22157fe7842ea88343e75f32c106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a6c0be702a491f9c0538441ef740f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea2aa6b065e42148fd153465361bf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970fae4272514a7ab7e61802ab9a3cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c61340a69a4dc9b5c39506ebaa6bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "mrpc_dataset = load_dataset('glue', name='mrpc', split='train')\n",
    "print(mrpc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff9c8ef-57f4-4c84-8474-633aca4eac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value(dtype='int32', id=None),\n",
      " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
      " 'sentence1': Value(dtype='string', id=None),\n",
      " 'sentence2': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(mrpc_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a516e-acf7-4af5-9ea2-1789660fd096",
   "metadata": {},
   "source": [
    "### Common methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc413e6b-9e4a-4a22-88bc-d5bb62328f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06c9092-ac34-454d-9bdd-0e3eaff5609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20f3f861-dd35-4b5e-a2c5-b358189ab782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbdaecf38b04ba691b592e0c470ceea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddf71227b524dbfa7e23dad7c710d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 22074\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 21909\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "num_words = 100\n",
    "imdb_filtered = imdb_dataset.filter(lambda ex: len(ex['text'].split(' ')) >= num_words)\n",
    "print(imdb_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54b51ddb-288d-44f7-8828-f041cae3db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(ex):\n",
    "    ex['text'] = 'IMDB: ' + ex['text']\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c32843da-86c5-4b0c-8163-56234825fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c365158c823146e0b6cf2dd8fbc2b33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66c9013c84a43f4a538a0ceb710e086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_prefixed = imdb_dataset.map(add_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957c1e3c-8323-498e-8340-a679cad8f158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IMDB: Although I have to admit I laughed more watching this movie than the last few comedies I saw.<br /><br />The budget must have consisted of pocket change from the actors. The production values are so low that they actual made it kind of fun to watch. Reminds me of the Robot Monster made up of a guy in a gorilla suit with a cardboard diving helmet on.<br /><br />In one scene a hapless victim gets their arm and leg cut off. Geez, hard to believe but the Black Knight scene from Holy Grail was more realistic. I kept wondering why the victim didn\\'t start shouting \" None Shall Pass\" and \" It\\'s only a flesh wound, I\\'ve had worse\". It was one of the funniest scenes I\\'ve seen in the past year.<br /><br />The \"gladiator/demon\" was a stitch too. Between the horribly cheap costume and the geeky look of the guy in it the end result was hysterical.<br /><br />Truly a movie that is bad enough to be watchable. Kind of like seeing a slow motion auto accident on film.<br /><br />'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_prefixed['train']['text'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0480b60b-55a4-4154-bc10-aba45607f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_whole = load_dataset('stanfordnlp/imdb', split='train+test')\n",
    "print(imdb_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d083cad0-311a-4b6c-8bf2-fb6620a5ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c858ed8f2b64987993a463255b1cd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749a46bf7d4f4e7d8222e7e00629ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc13e3e57d944e78a8fb1d30b385273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18501c25af3e4354a5392979064db247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ed67849744437eb0fef977967c5752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3d8f19b0174493a1c17cfe37ce6b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6db27a61bb4a81aad83b3886ccae61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 10662\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "rt_dataset = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='all')\n",
    "print(rt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aa97e29-998c-423b-859d-0f986e969f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 60662\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "concatenated = concatenate_datasets([imdb_whole, rt_dataset])\n",
    "print(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4eb0dd8b-947a-4e35-a5c7-d099acf17f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import interleave_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b53a21c-3729-4f7f-973a-37041ebddf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 26450\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "inter = interleave_datasets([imdb_whole, rt_dataset], probabilities=[0.6, 0.4], seed=42)\n",
    "print(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3c6c90f-bc0d-45bc-9f19-5275cfe393a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['text', 'label'],\n",
      "    n_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_iter = load_dataset('stanfordnlp/imdb', split='train', streaming=True)\n",
    "print(imdb_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bd0c90d-fbb9-45a8-8aff-7aa64dabe1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the '\n",
      "         'controversy that surrounded it when it was first released in 1967. I '\n",
      "         'also heard that at first it was seized by U.S. customs if it ever '\n",
      "         'tried to enter this country, therefore being a fan of films '\n",
      "         'considered \"controversial\" I really had to see this for myself.<br '\n",
      "         '/><br />The plot is centered around a young Swedish drama student '\n",
      "         'named Lena who wants to learn everything she can about life. In '\n",
      "         'particular she wants to focus her attentions to making some sort of '\n",
      "         'documentary on what the average Swede thought about certain '\n",
      "         'political issues such as the Vietnam War and race issues in the '\n",
      "         'United States. In between asking politicians and ordinary denizens '\n",
      "         'of Stockholm about their opinions on politics, she has sex with her '\n",
      "         'drama teacher, classmates, and married men.<br /><br />What kills me '\n",
      "         'about I AM CURIOUS-YELLOW is that 40 years ago, this was considered '\n",
      "         'pornographic. Really, the sex and nudity scenes are few and far '\n",
      "         \"between, even then it's not shot like some cheaply made porno. While \"\n",
      "         'my countrymen mind find it shocking, in reality sex and nudity are a '\n",
      "         'major staple in Swedish cinema. Even Ingmar Bergman, arguably their '\n",
      "         'answer to good old boy John Ford, had sex scenes in his films.<br '\n",
      "         '/><br />I do commend the filmmakers for the fact that any sex shown '\n",
      "         'in the film is shown for artistic purposes rather than just to shock '\n",
      "         'people and make money to be shown in pornographic theaters in '\n",
      "         'America. I AM CURIOUS-YELLOW is a good film for anyone wanting to '\n",
      "         'study the meat and potatoes (no pun intended) of Swedish cinema. But '\n",
      "         \"really, this film doesn't have much of a plot.\"}\n"
     ]
    }
   ],
   "source": [
    "for ex in imdb_iter:\n",
    "    pprint(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ad4b19c-455e-4598-aec6-45075b5e9cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverification_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVerificationMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load a dataset from the Hugging Face Hub, or a local dataset.\n",
       "\n",
       "You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
       "\n",
       "A dataset is a directory that contains:\n",
       "\n",
       "- some data files in generic formats (JSON, CSV, Parquet, text, etc.).\n",
       "- and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n",
       "\n",
       "Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n",
       "\n",
       "This function does the following under the hood:\n",
       "\n",
       "    1. Download and import in the library the dataset script from `path` if it's not already cached inside the library.\n",
       "\n",
       "        If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n",
       "\n",
       "        Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n",
       "        contain the path or URL to the original data files and the code to load examples from the original data files.\n",
       "\n",
       "        You can find the complete list of datasets in the Datasets [Hub](https://huggingface.co/datasets).\n",
       "\n",
       "    2. Run the dataset script which will:\n",
       "\n",
       "        * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n",
       "        * Process and cache the dataset in typed Arrow tables for caching.\n",
       "\n",
       "            Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
       "            They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
       "\n",
       "    3. Return a dataset built from the requested splits in `split` (default: all).\n",
       "\n",
       "It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n",
       "In this case, it automatically loads all the data files from the directory or the dataset repository.\n",
       "\n",
       "Args:\n",
       "\n",
       "    path (`str`):\n",
       "        Path or name of the dataset.\n",
       "        Depending on `path`, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n",
       "\n",
       "        For local datasets:\n",
       "\n",
       "        - if `path` is a local directory (containing data files only)\n",
       "          -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n",
       "          e.g. `'./path/to/directory/with/my/csv/data'`.\n",
       "        - if `path` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory)\n",
       "          -> load the dataset builder from the dataset script\n",
       "          e.g. `'./dataset/squad'` or `'./dataset/squad/squad.py'`.\n",
       "\n",
       "        For datasets on the Hugging Face Hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
       "\n",
       "        - if `path` is a dataset repository on the HF hub (containing data files only)\n",
       "          -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n",
       "          e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing your data files.\n",
       "        - if `path` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n",
       "          -> load the dataset builder from the dataset script in the dataset repository\n",
       "          e.g. `glue`, `squad`, `'username/dataset_name'`, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n",
       "\n",
       "    name (`str`, *optional*):\n",
       "        Defining the name of the dataset configuration.\n",
       "    data_dir (`str`, *optional*):\n",
       "        Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
       "        the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
       "    data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
       "        Path(s) to source data file(s).\n",
       "    split (`Split` or `str`):\n",
       "        Which split of the data to load.\n",
       "        If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
       "        If given, will return a single Dataset.\n",
       "        Splits can be combined and specified like in tensorflow-datasets.\n",
       "    cache_dir (`str`, *optional*):\n",
       "        Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
       "    features (`Features`, *optional*):\n",
       "        Set the features type to use for this dataset.\n",
       "    download_config ([`DownloadConfig`], *optional*):\n",
       "        Specific download configuration parameters.\n",
       "    download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
       "        Download/generate mode.\n",
       "    verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
       "        Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
       "\n",
       "        <Added version=\"2.9.1\"/>\n",
       "    ignore_verifications (`bool`, defaults to `False`):\n",
       "        Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n",
       "\n",
       "        <Deprecated version=\"2.9.1\">\n",
       "\n",
       "        `ignore_verifications` was deprecated in version 2.9.1 and will be removed in 3.0.0.\n",
       "        Please use `verification_mode` instead.\n",
       "\n",
       "        </Deprecated>\n",
       "    keep_in_memory (`bool`, defaults to `None`):\n",
       "        Whether to copy the dataset in-memory. If `None`, the dataset\n",
       "        will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
       "        nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
       "    save_infos (`bool`, defaults to `False`):\n",
       "        Save the dataset information (checksums/size/splits/...).\n",
       "    revision ([`Version`] or `str`, *optional*):\n",
       "        Version of the dataset script to load.\n",
       "        As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
       "        You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
       "    token (`str` or `bool`, *optional*):\n",
       "        Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
       "        If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
       "    use_auth_token (`str` or `bool`, *optional*):\n",
       "        Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
       "        If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
       "\n",
       "        <Deprecated version=\"2.14.0\">\n",
       "\n",
       "        `use_auth_token` was deprecated in favor of `token` in version 2.14.0 and will be removed in 3.0.0.\n",
       "\n",
       "        </Deprecated>\n",
       "    task (`str`):\n",
       "        The task to prepare the dataset for during training and evaluation. Casts the dataset's [`Features`] to standardized column names and types as detailed in `datasets.tasks`.\n",
       "\n",
       "        <Deprecated version=\"2.13.0\">\n",
       "\n",
       "        `task` was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
       "\n",
       "        </Deprecated>\n",
       "    streaming (`bool`, defaults to `False`):\n",
       "        If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
       "        iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
       "\n",
       "        Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
       "        Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
       "        like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
       "    num_proc (`int`, *optional*, defaults to `None`):\n",
       "        Number of processes when downloading and generating the dataset locally.\n",
       "        Multiprocessing is disabled by default.\n",
       "\n",
       "        <Added version=\"2.7.0\"/>\n",
       "    storage_options (`dict`, *optional*, defaults to `None`):\n",
       "        **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
       "\n",
       "        <Added version=\"2.11.0\"/>\n",
       "    trust_remote_code (`bool`, defaults to `False`):\n",
       "        Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
       "        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
       "        execute code present on the Hub on your local machine.\n",
       "\n",
       "        <Added version=\"2.16.0\"/>\n",
       "\n",
       "        <Changed version=\"2.20.0\">\n",
       "\n",
       "        `trust_remote_code` defaults to `False` if not specified.\n",
       "\n",
       "        </Changed>\n",
       "\n",
       "    **config_kwargs (additional keyword arguments):\n",
       "        Keyword arguments to be passed to the `BuilderConfig`\n",
       "        and used in the [`DatasetBuilder`].\n",
       "\n",
       "Returns:\n",
       "    [`Dataset`] or [`DatasetDict`]:\n",
       "    - if `split` is not `None`: the dataset requested,\n",
       "    - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
       "\n",
       "    or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
       "\n",
       "    - if `split` is not `None`, the dataset is requested\n",
       "    - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
       "\n",
       "Example:\n",
       "\n",
       "Load a dataset from the Hugging Face Hub:\n",
       "\n",
       "```py\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('rotten_tomatoes', split='train')\n",
       "\n",
       "# Map data files to splits\n",
       ">>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
       ">>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
       "```\n",
       "\n",
       "Load a local dataset:\n",
       "\n",
       "```py\n",
       "# Load a CSV file\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
       "\n",
       "# Load a JSON file\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
       "\n",
       "# Load from a local loading script\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
       "```\n",
       "\n",
       "Load an [`~datasets.IterableDataset`]:\n",
       "\n",
       "```py\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n",
       "```\n",
       "\n",
       "Load an image dataset with the `ImageFolder` dataset builder:\n",
       "\n",
       "```py\n",
       ">>> from datasets import load_dataset\n",
       ">>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      ~/conda/envs/dlp/lib/python3.12/site-packages/datasets/load.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c5622-8c46-4e4e-83f5-edfd0a01f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
