{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd46566-8be2-48bf-9bbd-60ec77f71511",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d99f144a75e42d8baaed20d27207150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34a72bd108b46f2b125dd26f2c0bb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66e32df067b4834a7ed92812a03ebc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3465d7c874924e5bae4c0d7349a50a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/74004228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 74004228\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('bookcorpus', split='all')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2952bb76-b942-4549-a12c-96eb04be47f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usually , he would be tearing around the living room , playing with his toys .',\n",
       " 'but just one look at a minion sent him practically catatonic .',\n",
       " \"that had been megan 's plan when she got him dressed earlier .\",\n",
       " \"he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\",\n",
       " 'she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .',\n",
       " \"`` are n't you being a good boy ? ''\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text'][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98635432-4e9b-49b6-8f69-cedc546a9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d57ac84-be28-4327-965c-1512e8a5dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "model = BPE(unk_token=\"[UNK]\")\n",
    "tokenizer = Tokenizer(model)\n",
    "\n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a385d7bc-4176-4fa7-abb2-fdb6dc10d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=32_000, special_tokens=[\"[PAD]\", \"[UNK]\"], continuing_subword_prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74442eb-e2e5-488f-a001-df2ec9a2c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(batch_size=1_000):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i:(i + batch_size)]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238b0ac4-10dd-44bb-9f36-ed9fd9e37ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = tokenizer.train_from_iterator(get_examples(), trainer, len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33dcda3a-77bd-4c13-ad36-491d6f27435b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhopper\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "tokenizer.model.save('model', prefix='hopper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d37c71-aaf3-48ce-a191-da38081dce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45cad4d-8d33-420d-ac02-96d416af6084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/hopper-vocab.json', 'model/hopper-merges.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save('model', prefix='hopper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c53578-a25d-4bce-a707-3fffc74751d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#version: 0.2\n",
      "##h ##e\n",
      "t ##he\n",
      "##i ##n\n",
      "##e ##r\n",
      "##e ##d\n",
      "##o ##u\n",
      "##n ##d\n",
      "##in ##g\n",
      "t ##o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head -n10 model/hopper-merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7f7764-0a95-4651-8c78-2ebd28ab1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel ##anthe\n",
      "black ##er\n",
      "ad ##ject\n",
      "v ##ang\n",
      "betroth ##al\n",
      "tiptoe ##ing\n",
      "restroom ##s\n",
      "consol ##ing\n",
      "esp ##ionage\n",
      "influ ##x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tail -n10 model/hopper-merges.txt | tac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6abc228-4cf0-4bfa-80a3-cd4e3461baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31871 model/hopper-merges.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wc -l model/hopper-merges.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe43311-1577-4622-8421-98501262e35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "140fe269-6e46-4436-84f0-5ec7b56d6c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30b78b42-ec3b-4a50-89f4-c95fe0a25f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[UNK]', 1),\n",
       " ('\\x13', 2),\n",
       " ('\\x14', 3),\n",
       " ('\\x18', 4),\n",
       " ('\\x19', 5),\n",
       " ('\\x1c', 6),\n",
       " ('\\x1d', 7),\n",
       " ('\\x1f', 8),\n",
       " ('!', 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])\n",
    "vocab_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e1a7d-ee21-4618-a5ed-9aafc1050132",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dd00cb1-150a-4a5e-8eaa-e43f78bbde03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usually , he would be tearing around the living room , playing with his toys .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ds[0]['text']\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52429fff-7b95-4607-82ec-fb85666cec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4846410a-cf71-4fa6-928b-a791d34da3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = encoding.ids\n",
    "tokens = encoding.tokens\n",
    "type_ids = encoding.type_ids\n",
    "attention_mask = encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bedfb88d-1f5f-4c51-8955-7df3d97fbec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "        <head>\n",
       "            <style>\n",
       "                .tokenized-text {\n",
       "    width:100%;\n",
       "    padding:2rem;\n",
       "    max-height: 400px;\n",
       "    overflow-y: auto;\n",
       "    box-sizing:border-box;\n",
       "    line-height:4rem; /* Lots of space between lines */\n",
       "    font-family: \"Roboto Light\", \"Ubuntu Light\", \"Ubuntu\", monospace;\n",
       "    box-shadow: 2px 2px 2px rgba(0,0,0,0.2);\n",
       "    background-color: rgba(0,0,0,0.01);\n",
       "    letter-spacing:2px; /* Give some extra separation between chars */\n",
       "}\n",
       ".non-token{\n",
       "    /* White space and other things the tokenizer ignores*/\n",
       "    white-space: pre;\n",
       "    letter-spacing:4px;\n",
       "    border-top:1px solid #A0A0A0; /* A gentle border on top and bottom makes tabs more ovious*/\n",
       "    border-bottom:1px solid #A0A0A0;\n",
       "    line-height: 1rem;\n",
       "    height: calc(100% - 2px);\n",
       "}\n",
       "\n",
       ".token {\n",
       "    white-space: pre;\n",
       "    position:relative;\n",
       "    color:black;\n",
       "    letter-spacing:2px;\n",
       "}\n",
       "\n",
       ".annotation{\n",
       "    white-space:nowrap; /* Important - ensures that annotations appears even if the annotated text wraps a line */\n",
       "    border-radius:4px;\n",
       "    position:relative;\n",
       "    width:fit-content;\n",
       "}\n",
       ".annotation:before {\n",
       "    /*The before holds the text and the after holds the background*/\n",
       "    z-index:1000; /* Make sure this is above the background */\n",
       "    content:attr(data-label); /* The annotations label is on a data attribute */\n",
       "    color:white;\n",
       "    position:absolute;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    left:0;\n",
       "    width:100%;\n",
       "    padding:0.5rem 0;\n",
       "    /* These make it so an annotation doesn't stretch beyond the annotated text if the label is longer*/\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    text-overflow:ellipsis;\n",
       "}\n",
       "\n",
       ".annotation:after {\n",
       "    content:attr(data-label); /* The content defines the width of the annotation*/\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "\n",
       "    left:0;\n",
       "    width:100%; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "\n",
       "    padding:0.5rem 0;\n",
       "    /* Nast hack below:\n",
       "    We set the annotations color in code because we don't know the colors at css time.\n",
       "    But you can't pass a color as a data attribute to get it into the pseudo element (this thing)\n",
       "    So to get around that, annotations have the color set on them with a style attribute and then we\n",
       "    can get the color with currentColor.\n",
       "    Annotations wrap tokens and tokens set the color back to black\n",
       "     */\n",
       "    background-color: currentColor;\n",
       "}\n",
       ".annotation:hover::after, .annotation:hover::before{\n",
       "    /* When the user hovers over an annotation expand the label to display in full\n",
       "     */\n",
       "    min-width: fit-content;\n",
       "}\n",
       "\n",
       ".annotation:hover{\n",
       "    /* Emphasize the annotation start end with a border on hover*/\n",
       "    border-color: currentColor;\n",
       "    border: 2px solid;\n",
       "}\n",
       ".special-token:not(:empty){\n",
       "    /*\n",
       "    A none empty special token is like UNK (as opposed to CLS which has no representation in the text )\n",
       "     */\n",
       "    position:relative;\n",
       "}\n",
       ".special-token:empty::before{\n",
       "    /* Special tokens that don't have text are displayed as pseudo elements so we dont select them with the mouse*/\n",
       "    content:attr(data-stok);\n",
       "    background:#202020;\n",
       "    font-size:0.75rem;\n",
       "    color:white;\n",
       "    margin: 0 0.25rem;\n",
       "    padding: 0.25rem;\n",
       "    border-radius:4px\n",
       "}\n",
       "\n",
       ".special-token:not(:empty):before {\n",
       "    /* Special tokens that have text (UNK) are displayed above the actual text*/\n",
       "    content:attr(data-stok);\n",
       "    position:absolute;\n",
       "    bottom:1.75rem;\n",
       "    min-width:100%;\n",
       "    width:100%;\n",
       "    height:1rem;\n",
       "    line-height:1rem;\n",
       "    font-size:1rem;\n",
       "    text-align:center;\n",
       "    color:white;\n",
       "    font-weight:bold;\n",
       "    background:#202020;\n",
       "    border-radius:10%;\n",
       "}\n",
       "/*\n",
       "We want to alternate the color of tokens, but we can't use nth child because tokens might be broken up by annotations\n",
       "instead we apply even and odd class at generation time and color them that way\n",
       " */\n",
       ".even-token{\n",
       "    background:#DCDCDC\t;\n",
       "    border: 1px solid #DCDCDC;\n",
       "}\n",
       ".odd-token{\n",
       "    background:#A0A0A0;\n",
       "    border: 1px solid #A0A0A0;\n",
       "}\n",
       ".even-token.multi-token,.odd-token.multi-token{\n",
       "    background:  repeating-linear-gradient(\n",
       "    45deg,\n",
       "    transparent,\n",
       "    transparent 1px,\n",
       "    #ccc 1px,\n",
       "    #ccc 1px\n",
       "    ),\n",
       "    /* on \"bottom\" */\n",
       "    linear-gradient(\n",
       "    to bottom,\n",
       "    #FFB6C1,\n",
       "    #999\n",
       "    );\n",
       "}\n",
       "\n",
       ".multi-token:hover::after {\n",
       "    content:\"This char has more than 1 token\"; /* The content defines the width of the annotation*/\n",
       "    color:white;\n",
       "    background-color: black;\n",
       "    position:absolute;\n",
       "    font-size:0.75rem;\n",
       "    text-align:center;\n",
       "    font-weight:bold;\n",
       "    text-overflow:ellipsis;\n",
       "    top:1.75rem;\n",
       "    line-height:0;\n",
       "    overflow: hidden;\n",
       "    white-space: nowrap;\n",
       "    left:0;\n",
       "    width:fit-content; /* 100% of the parent, which is the annotation whose width is the tokens inside it*/\n",
       "    padding:0.5rem 0;\n",
       "}\n",
       "\n",
       "            </style>\n",
       "        </head>\n",
       "        <body>\n",
       "            <div class=\"tokenized-text\" dir=auto>\n",
       "            <span class=\"token even-token\"  >usually</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >he</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >would</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >be</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >tearing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >around</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >the</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >living</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >room</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >,</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >playing</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >with</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >his</span><span class=\"non-token\"  > </span><span class=\"token even-token\"  >toys</span><span class=\"non-token\"  > </span><span class=\"token odd-token\"  >.</span>\n",
       "            </div>\n",
       "        </body>\n",
       "    </html>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers.tools import EncodingVisualizer\n",
    "viz = EncodingVisualizer(tokenizer=tokenizer)\n",
    "viz(text=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5072544-1f32-40fe-b8fd-e7f73e10fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ids</th>\n",
       "      <th>type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually</td>\n",
       "      <td>2462</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>would</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tearing</td>\n",
       "      <td>6456</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>around</td>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>living</td>\n",
       "      <td>1559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>room</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>playing</td>\n",
       "      <td>2301</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>with</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>his</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>toys</td>\n",
       "      <td>9774</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens   ids  type_ids  attention_mask\n",
       "0   usually  2462         0               1\n",
       "1         ,    19         0               1\n",
       "2        he   149         0               1\n",
       "3     would   277         0               1\n",
       "4        be   162         0               1\n",
       "5   tearing  6456         0               1\n",
       "6    around   422         0               1\n",
       "7       the   131         0               1\n",
       "8    living  1559         0               1\n",
       "9      room   536         0               1\n",
       "10        ,    19         0               1\n",
       "11  playing  2301         0               1\n",
       "12     with   201         0               1\n",
       "13      his   177         0               1\n",
       "14     toys  9774         0               1\n",
       "15        .    21         0               1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "out_dict = {'tokens': tokens, 'ids': token_ids, 'type_ids': type_ids, 'attention_mask': attention_mask}\n",
    "df = pd.DataFrame.from_dict(out_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de52db91-e34b-4d24-b0c6-6607e7b274fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83f1aaa8-6bba-46b3-9dd1-03eb6db6c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ds[:4]['text']\n",
    "batch_enc = tokenizer.encode_batch(samples)\n",
    "batch_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbb82c9f-ad0c-47d5-9375-19cea64158d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding hasn't happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "684539ed-c071-43d1-a219-18a7fdf43570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(direction='right', pad_id=0, pad_type_id=0, pad_token='[PAD]', length=None, pad_to_multiple_of=None)\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "905e63e2-a8d4-4739-801c-55d30545a053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=42, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_batch(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3645fc41-076c-4ada-87a9-a48f57b0e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '[UNK]', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "text = \"All this is so simple to do in HF இ😊.\"\n",
    "enc = tokenizer.encode(text)\n",
    "print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a0c7cc9-e15c-4719-b877-ccc3cfdce673",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('hopper.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3841dd0c-88b4-4043-b0bb-4bc703a472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d8489e-fb62-49b4-bff5-6d12c2822b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '1.0',\n",
       " 'truncation': {'direction': 'Right',\n",
       "  'max_length': 512,\n",
       "  'strategy': 'LongestFirst',\n",
       "  'stride': 0},\n",
       " 'padding': {'strategy': 'BatchLongest',\n",
       "  'direction': 'Right',\n",
       "  'pad_to_multiple_of': None,\n",
       "  'pad_id': 0,\n",
       "  'pad_type_id': 0,\n",
       "  'pad_token': '[PAD]'},\n",
       " 'added_tokens': [{'id': 0,\n",
       "   'content': '[PAD]',\n",
       "   'single_word': False,\n",
       "   'lstrip': False,\n",
       "   'rstrip': False,\n",
       "   'normalized': False,\n",
       "   'special': True},\n",
       "  {'id': 1,\n",
       "   'content': '[UNK]',\n",
       "   'single_word': False,\n",
       "   'lstrip': False,\n",
       "   'rstrip': False,\n",
       "   'normalized': False,\n",
       "   'special': True}],\n",
       " 'normalizer': {'type': 'Lowercase'},\n",
       " 'pre_tokenizer': {'type': 'Whitespace'},\n",
       " 'post_processor': None,\n",
       " 'decoder': None,\n",
       " 'model': {'type': 'BPE',\n",
       "  'dropout': None,\n",
       "  'unk_token': '[UNK]',\n",
       "  'continuing_subword_prefix': '##',\n",
       "  'end_of_word_suffix': None,\n",
       "  'fuse_unk': False,\n",
       "  'byte_fallback': False,\n",
       "  'ignore_merges': False,\n",
       "  'vocab': {'[PAD]': 0,\n",
       "   '[UNK]': 1,\n",
       "   '\\x13': 2,\n",
       "   '\\x14': 3,\n",
       "   '\\x18': 4,\n",
       "   '\\x19': 5,\n",
       "   '\\x1c': 6,\n",
       "   '\\x1d': 7,\n",
       "   '\\x1f': 8,\n",
       "   '!': 9,\n",
       "   '#': 10,\n",
       "   '$': 11,\n",
       "   '%': 12,\n",
       "   '&': 13,\n",
       "   \"'\": 14,\n",
       "   '(': 15,\n",
       "   ')': 16,\n",
       "   '*': 17,\n",
       "   '+': 18,\n",
       "   ',': 19,\n",
       "   '-': 20,\n",
       "   '.': 21,\n",
       "   '/': 22,\n",
       "   '0': 23,\n",
       "   '1': 24,\n",
       "   '2': 25,\n",
       "   '3': 26,\n",
       "   '4': 27,\n",
       "   '5': 28,\n",
       "   '6': 29,\n",
       "   '7': 30,\n",
       "   '8': 31,\n",
       "   '9': 32,\n",
       "   ':': 33,\n",
       "   ';': 34,\n",
       "   '<': 35,\n",
       "   '=': 36,\n",
       "   '>': 37,\n",
       "   '?': 38,\n",
       "   '@': 39,\n",
       "   '[': 40,\n",
       "   '\\\\': 41,\n",
       "   ']': 42,\n",
       "   '^': 43,\n",
       "   '_': 44,\n",
       "   '`': 45,\n",
       "   'a': 46,\n",
       "   'b': 47,\n",
       "   'c': 48,\n",
       "   'd': 49,\n",
       "   'e': 50,\n",
       "   'f': 51,\n",
       "   'g': 52,\n",
       "   'h': 53,\n",
       "   'i': 54,\n",
       "   'j': 55,\n",
       "   'k': 56,\n",
       "   'l': 57,\n",
       "   'm': 58,\n",
       "   'n': 59,\n",
       "   'o': 60,\n",
       "   'p': 61,\n",
       "   'q': 62,\n",
       "   'r': 63,\n",
       "   's': 64,\n",
       "   't': 65,\n",
       "   'u': 66,\n",
       "   'v': 67,\n",
       "   'w': 68,\n",
       "   'x': 69,\n",
       "   'y': 70,\n",
       "   'z': 71,\n",
       "   '{': 72,\n",
       "   '|': 73,\n",
       "   '}': 74,\n",
       "   '~': 75,\n",
       "   '\\x7f': 76,\n",
       "   '##n': 77,\n",
       "   '##o': 78,\n",
       "   '##c': 79,\n",
       "   '##k': 80,\n",
       "   '##w': 81,\n",
       "   '##i': 82,\n",
       "   '##l': 83,\n",
       "   '##t': 84,\n",
       "   '##r': 85,\n",
       "   '##e': 86,\n",
       "   '##d': 87,\n",
       "   '##s': 88,\n",
       "   '##h': 89,\n",
       "   '##u': 90,\n",
       "   '##x': 91,\n",
       "   '##f': 92,\n",
       "   '##a': 93,\n",
       "   '##p': 94,\n",
       "   '##v': 95,\n",
       "   '##y': 96,\n",
       "   '##m': 97,\n",
       "   '##z': 98,\n",
       "   '##g': 99,\n",
       "   '##b': 100,\n",
       "   '##j': 101,\n",
       "   '##q': 102,\n",
       "   '##3': 103,\n",
       "   '##8': 104,\n",
       "   '##0': 105,\n",
       "   '##4': 106,\n",
       "   '##1': 107,\n",
       "   '##5': 108,\n",
       "   '##6': 109,\n",
       "   '##7': 110,\n",
       "   '##2': 111,\n",
       "   '##_': 112,\n",
       "   '##9': 113,\n",
       "   '##*': 114,\n",
       "   '##\\\\': 115,\n",
       "   '##~': 116,\n",
       "   '##-': 117,\n",
       "   '##`': 118,\n",
       "   '##=': 119,\n",
       "   '##|': 120,\n",
       "   '##+': 121,\n",
       "   '##.': 122,\n",
       "   '##/': 123,\n",
       "   \"##'\": 124,\n",
       "   '##\\x19': 125,\n",
       "   '##^': 126,\n",
       "   '##:': 127,\n",
       "   '##\\x1d': 128,\n",
       "   '##,': 129,\n",
       "   '##he': 130,\n",
       "   'the': 131,\n",
       "   '##in': 132,\n",
       "   '##er': 133,\n",
       "   '##ed': 134,\n",
       "   '##ou': 135,\n",
       "   '##nd': 136,\n",
       "   '##ing': 137,\n",
       "   'to': 138,\n",
       "   '##at': 139,\n",
       "   '##re': 140,\n",
       "   'th': 141,\n",
       "   '##is': 142,\n",
       "   'and': 143,\n",
       "   '##as': 144,\n",
       "   '##en': 145,\n",
       "   '##an': 146,\n",
       "   '##on': 147,\n",
       "   '##ll': 148,\n",
       "   'he': 149,\n",
       "   '##ar': 150,\n",
       "   '##or': 151,\n",
       "   '##es': 152,\n",
       "   'of': 153,\n",
       "   'in': 154,\n",
       "   '##it': 155,\n",
       "   '``': 156,\n",
       "   \"''\": 157,\n",
       "   'ha': 158,\n",
       "   '##om': 159,\n",
       "   'you': 160,\n",
       "   '##ow': 161,\n",
       "   'be': 162,\n",
       "   'her': 163,\n",
       "   '##id': 164,\n",
       "   '##ut': 165,\n",
       "   'was': 166,\n",
       "   'it': 167,\n",
       "   '##ot': 168,\n",
       "   '##le': 169,\n",
       "   '##ld': 170,\n",
       "   '##ac': 171,\n",
       "   '##gh': 172,\n",
       "   'on': 173,\n",
       "   'she': 174,\n",
       "   '##et': 175,\n",
       "   'st': 176,\n",
       "   'his': 177,\n",
       "   'that': 178,\n",
       "   '##ly': 179,\n",
       "   '##im': 180,\n",
       "   '##ay': 181,\n",
       "   '##st': 182,\n",
       "   '##ve': 183,\n",
       "   '##ic': 184,\n",
       "   'wh': 185,\n",
       "   'as': 186,\n",
       "   're': 187,\n",
       "   'my': 188,\n",
       "   'me': 189,\n",
       "   '##ad': 190,\n",
       "   '##al': 191,\n",
       "   '##se': 192,\n",
       "   '##oo': 193,\n",
       "   '##ver': 194,\n",
       "   '##ght': 195,\n",
       "   '##ould': 196,\n",
       "   '##ur': 197,\n",
       "   '##ith': 198,\n",
       "   '##ir': 199,\n",
       "   'for': 200,\n",
       "   'with': 201,\n",
       "   '##ent': 202,\n",
       "   'do': 203,\n",
       "   'an': 204,\n",
       "   '##ke': 205,\n",
       "   'had': 206,\n",
       "   'li': 207,\n",
       "   'at': 208,\n",
       "   'sh': 209,\n",
       "   '##am': 210,\n",
       "   'we': 211,\n",
       "   'but': 212,\n",
       "   '##ion': 213,\n",
       "   'him': 214,\n",
       "   '##all': 215,\n",
       "   'fr': 216,\n",
       "   '##her': 217,\n",
       "   'se': 218,\n",
       "   'sa': 219,\n",
       "   'not': 220,\n",
       "   '##ter': 221,\n",
       "   '##ight': 222,\n",
       "   '##ked': 223,\n",
       "   '##pp': 224,\n",
       "   'ne': 225,\n",
       "   '##ro': 226,\n",
       "   '##ack': 227,\n",
       "   'so': 228,\n",
       "   '##ain': 229,\n",
       "   '##ch': 230,\n",
       "   '##ill': 231,\n",
       "   '##ant': 232,\n",
       "   '##ce': 233,\n",
       "   '##il': 234,\n",
       "   'kn': 235,\n",
       "   'out': 236,\n",
       "   'what': 237,\n",
       "   '##ore': 238,\n",
       "   'is': 239,\n",
       "   '##el': 240,\n",
       "   'up': 241,\n",
       "   '##out': 242,\n",
       "   '##ust': 243,\n",
       "   '##rou': 244,\n",
       "   '##ri': 245,\n",
       "   'whe': 246,\n",
       "   'have': 247,\n",
       "   '##ome': 248,\n",
       "   'said': 249,\n",
       "   'they': 250,\n",
       "   'no': 251,\n",
       "   '##hing': 252,\n",
       "   '##ard': 253,\n",
       "   'this': 254,\n",
       "   'ch': 255,\n",
       "   'from': 256,\n",
       "   'go': 257,\n",
       "   'su': 258,\n",
       "   'did': 259,\n",
       "   'ab': 260,\n",
       "   '##ra': 261,\n",
       "   '##ct': 262,\n",
       "   'le': 263,\n",
       "   '##nt': 264,\n",
       "   '##king': 265,\n",
       "   '##ere': 266,\n",
       "   'all': 267,\n",
       "   '##ie': 268,\n",
       "   'de': 269,\n",
       "   'al': 270,\n",
       "   '##us': 271,\n",
       "   '##one': 272,\n",
       "   'could': 273,\n",
       "   '##ind': 274,\n",
       "   'loo': 275,\n",
       "   'back': 276,\n",
       "   'would': 277,\n",
       "   'were': 278,\n",
       "   '##to': 279,\n",
       "   '##hed': 280,\n",
       "   'like': 281,\n",
       "   'fe': 282,\n",
       "   'if': 283,\n",
       "   '##ess': 284,\n",
       "   '##art': 285,\n",
       "   '##ge': 286,\n",
       "   '##own': 287,\n",
       "   '##est': 288,\n",
       "   'one': 289,\n",
       "   'cl': 290,\n",
       "   'there': 291,\n",
       "   '##un': 292,\n",
       "   '##ood': 293,\n",
       "   'sp': 294,\n",
       "   'con': 295,\n",
       "   '##ss': 296,\n",
       "   'just': 297,\n",
       "   'ag': 298,\n",
       "   'into': 299,\n",
       "   '##ab': 300,\n",
       "   '##ast': 301,\n",
       "   '##au': 302,\n",
       "   'when': 303,\n",
       "   'wor': 304,\n",
       "   'ex': 305,\n",
       "   'know': 306,\n",
       "   '##ers': 307,\n",
       "   '##ry': 308,\n",
       "   'ro': 309,\n",
       "   '##ide': 310,\n",
       "   'about': 311,\n",
       "   '##ep': 312,\n",
       "   '##ck': 313,\n",
       "   '##ace': 314,\n",
       "   '##and': 315,\n",
       "   'bl': 316,\n",
       "   'or': 317,\n",
       "   'hand': 318,\n",
       "   '##ally': 319,\n",
       "   '##ake': 320,\n",
       "   'then': 321,\n",
       "   '##um': 322,\n",
       "   'see': 323,\n",
       "   'them': 324,\n",
       "   'your': 325,\n",
       "   'want': 326,\n",
       "   '##way': 327,\n",
       "   'over': 328,\n",
       "   '##ul': 329,\n",
       "   'te': 330,\n",
       "   'any': 331,\n",
       "   '##op': 332,\n",
       "   'are': 333,\n",
       "   '##ong': 334,\n",
       "   '##lf': 335,\n",
       "   'ey': 336,\n",
       "   'been': 337,\n",
       "   '##ist': 338,\n",
       "   '##ate': 339,\n",
       "   'sm': 340,\n",
       "   'get': 341,\n",
       "   '##ven': 342,\n",
       "   'tim': 343,\n",
       "   '##ap': 344,\n",
       "   'pl': 345,\n",
       "   'down': 346,\n",
       "   'lo': 347,\n",
       "   'co': 348,\n",
       "   'ever': 349,\n",
       "   'some': 350,\n",
       "   'mo': 351,\n",
       "   'too': 352,\n",
       "   '##ig': 353,\n",
       "   '##ame': 354,\n",
       "   'bo': 355,\n",
       "   'again': 356,\n",
       "   '##urn': 357,\n",
       "   '##ft': 358,\n",
       "   'thou': 359,\n",
       "   'com': 360,\n",
       "   '##are': 361,\n",
       "   'who': 362,\n",
       "   '##round': 363,\n",
       "   'man': 364,\n",
       "   'off': 365,\n",
       "   '##ation': 366,\n",
       "   '##ice': 367,\n",
       "   'ar': 368,\n",
       "   'br': 369,\n",
       "   'time': 370,\n",
       "   'now': 371,\n",
       "   'how': 372,\n",
       "   '##ink': 373,\n",
       "   'fl': 374,\n",
       "   'by': 375,\n",
       "   '##ff': 376,\n",
       "   'po': 377,\n",
       "   '##ous': 378,\n",
       "   'eyes': 379,\n",
       "   '##ag': 380,\n",
       "   'head': 381,\n",
       "   'us': 382,\n",
       "   'qu': 383,\n",
       "   'more': 384,\n",
       "   '##ick': 385,\n",
       "   '##ive': 386,\n",
       "   '##ort': 387,\n",
       "   '##ine': 388,\n",
       "   '##ull': 389,\n",
       "   'can': 390,\n",
       "   'sl': 391,\n",
       "   'en': 392,\n",
       "   '##ak': 393,\n",
       "   'sc': 394,\n",
       "   'tr': 395,\n",
       "   '##pped': 396,\n",
       "   '##ect': 397,\n",
       "   '##ther': 398,\n",
       "   '##other': 399,\n",
       "   'tw': 400,\n",
       "   '##ound': 401,\n",
       "   '##os': 402,\n",
       "   'than': 403,\n",
       "   'un': 404,\n",
       "   '##our': 405,\n",
       "   '##nder': 406,\n",
       "   '##itt': 407,\n",
       "   '##self': 408,\n",
       "   'even': 409,\n",
       "   'their': 410,\n",
       "   '##ved': 411,\n",
       "   'way': 412,\n",
       "   'im': 413,\n",
       "   '##fore': 414,\n",
       "   '##led': 415,\n",
       "   '##ile': 416,\n",
       "   '##dd': 417,\n",
       "   'before': 418,\n",
       "   'gr': 419,\n",
       "   '##ire': 420,\n",
       "   'every': 421,\n",
       "   'around': 422,\n",
       "   'will': 423,\n",
       "   '..': 424,\n",
       "   '##reat': 425,\n",
       "   'say': 426,\n",
       "   'going': 427,\n",
       "   'other': 428,\n",
       "   '##ass': 429,\n",
       "   '...': 430,\n",
       "   '##ure': 431,\n",
       "   'pre': 432,\n",
       "   '##ving': 433,\n",
       "   'here': 434,\n",
       "   'pro': 435,\n",
       "   'af': 436,\n",
       "   'som': 437,\n",
       "   'right': 438,\n",
       "   '##th': 439,\n",
       "   '##ade': 440,\n",
       "   'somet': 441,\n",
       "   'only': 442,\n",
       "   'think': 443,\n",
       "   '##rough': 444,\n",
       "   'bec': 445,\n",
       "   'turn': 446,\n",
       "   'through': 447,\n",
       "   'need': 448,\n",
       "   'gl': 449,\n",
       "   '##ue': 450,\n",
       "   '##ip': 451,\n",
       "   'looked': 452,\n",
       "   '##ear': 453,\n",
       "   '##ered': 454,\n",
       "   '##ose': 455,\n",
       "   'look': 456,\n",
       "   'pr': 457,\n",
       "   '##fe': 458,\n",
       "   'let': 459,\n",
       "   'thought': 460,\n",
       "   '##ite': 461,\n",
       "   'should': 462,\n",
       "   'sw': 463,\n",
       "   'thing': 464,\n",
       "   '##ward': 465,\n",
       "   '##act': 466,\n",
       "   '##iss': 467,\n",
       "   'still': 468,\n",
       "   'after': 469,\n",
       "   'away': 470,\n",
       "   '##ol': 471,\n",
       "   'gu': 472,\n",
       "   'kne': 473,\n",
       "   'something': 474,\n",
       "   'face': 475,\n",
       "   '##thing': 476,\n",
       "   'am': 477,\n",
       "   'door': 478,\n",
       "   '##pt': 479,\n",
       "   'mu': 480,\n",
       "   '##ont': 481,\n",
       "   'exp': 482,\n",
       "   '##ity': 483,\n",
       "   'good': 484,\n",
       "   '##ies': 485,\n",
       "   '##ause': 486,\n",
       "   'long': 487,\n",
       "   '##ang': 488,\n",
       "   'never': 489,\n",
       "   '##ment': 490,\n",
       "   'wat': 491,\n",
       "   'ac': 492,\n",
       "   '##ened': 493,\n",
       "   'pe': 494,\n",
       "   'wa': 495,\n",
       "   '##ach': 496,\n",
       "   '##dy': 497,\n",
       "   '##iz': 498,\n",
       "   'bet': 499,\n",
       "   'feel': 500,\n",
       "   'll': 501,\n",
       "   '##ble': 502,\n",
       "   'got': 503,\n",
       "   'well': 504,\n",
       "   'asked': 505,\n",
       "   'where': 506,\n",
       "   'che': 507,\n",
       "   '##able': 508,\n",
       "   'car': 509,\n",
       "   '##ark': 510,\n",
       "   'two': 511,\n",
       "   '##ree': 512,\n",
       "   'op': 513,\n",
       "   '##ated': 514,\n",
       "   'fo': 515,\n",
       "   '##wn': 516,\n",
       "   '##ance': 517,\n",
       "   '##ell': 518,\n",
       "   'fir': 519,\n",
       "   'arm': 520,\n",
       "   'made': 521,\n",
       "   '##ched': 522,\n",
       "   'why': 523,\n",
       "   '##ich': 524,\n",
       "   'much': 525,\n",
       "   've': 526,\n",
       "   '##med': 527,\n",
       "   '##ave': 528,\n",
       "   '##ather': 529,\n",
       "   '##ious': 530,\n",
       "   'our': 531,\n",
       "   'because': 532,\n",
       "   'tell': 533,\n",
       "   '##ting': 534,\n",
       "   'rem': 535,\n",
       "   'room': 536,\n",
       "   'knew': 537,\n",
       "   'dis': 538,\n",
       "   '##ittle': 539,\n",
       "   '##age': 540,\n",
       "   '##aking': 541,\n",
       "   '##ath': 542,\n",
       "   'under': 543,\n",
       "   'ho': 544,\n",
       "   'little': 545,\n",
       "   'mom': 546,\n",
       "   '##te': 547,\n",
       "   '##ning': 548,\n",
       "   '##ady': 549,\n",
       "   '##ough': 550,\n",
       "   'day': 551,\n",
       "   'cr': 552,\n",
       "   'ke': 553,\n",
       "   'make': 554,\n",
       "   '##ery': 555,\n",
       "   'come': 556,\n",
       "   '##co': 557,\n",
       "   'take': 558,\n",
       "   '##elt': 559,\n",
       "   '##gg': 560,\n",
       "   'call': 561,\n",
       "   'happ': 562,\n",
       "   'wal': 563,\n",
       "   'em': 564,\n",
       "   'may': 565,\n",
       "   '##ried': 566,\n",
       "   '##ase': 567,\n",
       "   '##ful': 568,\n",
       "   '##ks': 569,\n",
       "   'first': 570,\n",
       "   '##ress': 571,\n",
       "   'cont': 572,\n",
       "   '##side': 573,\n",
       "   '##be': 574,\n",
       "   '##ru': 575,\n",
       "   '##air': 576,\n",
       "   'vo': 577,\n",
       "   '##ber': 578,\n",
       "   'its': 579,\n",
       "   'felt': 580,\n",
       "   'jo': 581,\n",
       "   '##ence': 582,\n",
       "   'sure': 583,\n",
       "   'wo': 584,\n",
       "   'ca': 585,\n",
       "   '##ves': 586,\n",
       "   'start': 587,\n",
       "   'hands': 588,\n",
       "   '##ied': 589,\n",
       "   'tal': 590,\n",
       "   '##ans': 591,\n",
       "   '##ared': 592,\n",
       "   'hel': 593,\n",
       "   'pull': 594,\n",
       "   'wanted': 595,\n",
       "   'took': 596,\n",
       "   '##ah': 597,\n",
       "   'comp': 598,\n",
       "   '##int': 599,\n",
       "   'turned': 600,\n",
       "   '##em': 601,\n",
       "   'night': 602,\n",
       "   'el': 603,\n",
       "   'app': 604,\n",
       "   'sk': 605,\n",
       "   'really': 606,\n",
       "   '##ling': 607,\n",
       "   '##ps': 608,\n",
       "   'per': 609,\n",
       "   'des': 610,\n",
       "   'mar': 611,\n",
       "   '##oth': 612,\n",
       "   'try': 613,\n",
       "   '##ild': 614,\n",
       "   '##ily': 615,\n",
       "   'against': 616,\n",
       "   'voice': 617,\n",
       "   'sur': 618,\n",
       "   'has': 619,\n",
       "   'ad': 620,\n",
       "   '##uck': 621,\n",
       "   '##ia': 622,\n",
       "   'part': 623,\n",
       "   'res': 624,\n",
       "   'bel': 625,\n",
       "   '##ud': 626,\n",
       "   'fin': 627,\n",
       "   'hard': 628,\n",
       "   '##qu': 629,\n",
       "   'peop': 630,\n",
       "   'real': 631,\n",
       "   '##ced': 632,\n",
       "   'left': 633,\n",
       "   'which': 634,\n",
       "   '##ens': 635,\n",
       "   '##bb': 636,\n",
       "   'very': 637,\n",
       "   'beh': 638,\n",
       "   'help': 639,\n",
       "   'cou': 640,\n",
       "   'people': 641,\n",
       "   'came': 642,\n",
       "   'told': 643,\n",
       "   'another': 644,\n",
       "   '##ds': 645,\n",
       "   'last': 646,\n",
       "   'put': 647,\n",
       "   '##owed': 648,\n",
       "   '##ise': 649,\n",
       "   'side': 650,\n",
       "   'life': 651,\n",
       "   '##een': 652,\n",
       "   '##ook': 653,\n",
       "   '##ish': 654,\n",
       "   'while': 655,\n",
       "   '##ross': 656,\n",
       "   'body': 657,\n",
       "   'wom': 658,\n",
       "   'wait': 659,\n",
       "   '##ty': 660,\n",
       "   'find': 661,\n",
       "   'few': 662,\n",
       "   'sn': 663,\n",
       "   '##av': 664,\n",
       "   'spe': 665,\n",
       "   'gra': 666,\n",
       "   'anything': 667,\n",
       "   'rep': 668,\n",
       "   'moment': 669,\n",
       "   'yes': 670,\n",
       "   '##az': 671,\n",
       "   'being': 672,\n",
       "   '##les': 673,\n",
       "   'new': 674,\n",
       "   '##hes': 675,\n",
       "   'war': 676,\n",
       "   'breat': 677,\n",
       "   'nothing': 678,\n",
       "   '##ned': 679,\n",
       "   '##get': 680,\n",
       "   'ph': 681,\n",
       "   '##xt': 682,\n",
       "   'str': 683,\n",
       "   'own': 684,\n",
       "   'di': 685,\n",
       "   'behind': 686,\n",
       "   'ste': 687,\n",
       "   'keep': 688,\n",
       "   'unt': 689,\n",
       "   '##ions': 690,\n",
       "   'year': 691,\n",
       "   'enough': 692,\n",
       "   'toward': 693,\n",
       "   'went': 694,\n",
       "   'light': 695,\n",
       "   'lau': 696,\n",
       "   'does': 697,\n",
       "   'mat': 698,\n",
       "   'might': 699,\n",
       "   '##ouse': 700,\n",
       "   'saw': 701,\n",
       "   'wr': 702,\n",
       "   'hair': 703,\n",
       "   'bed': 704,\n",
       "   'dr': 705,\n",
       "   '##ci': 706,\n",
       "   '##igh': 707,\n",
       "   '##per': 708,\n",
       "   'beg': 709,\n",
       "   'dark': 710,\n",
       "   'clos': 711,\n",
       "   'sound': 712,\n",
       "   'gir': 713,\n",
       "   'old': 714,\n",
       "   'hu': 715,\n",
       "   'until': 716,\n",
       "   'things': 717,\n",
       "   'love': 718,\n",
       "   'mind': 719,\n",
       "   'cle': 720,\n",
       "   '##outh': 721,\n",
       "   '##ways': 722,\n",
       "   'though': 723,\n",
       "   'id': 724,\n",
       "   '##ars': 725,\n",
       "   'maybe': 726,\n",
       "   'bu': 727,\n",
       "   'those': 728,\n",
       "   'set': 729,\n",
       "   '##amp': 730,\n",
       "   '##ching': 731,\n",
       "   '##ail': 732,\n",
       "   'open': 733,\n",
       "   'end': 734,\n",
       "   '##ys': 735,\n",
       "   'act': 736,\n",
       "   '##aring': 737,\n",
       "   'looking': 738,\n",
       "   'fing': 739,\n",
       "   '##ness': 740,\n",
       "   'once': 741,\n",
       "   '##ng': 742,\n",
       "   '##ord': 743,\n",
       "   '##ost': 744,\n",
       "   '##orm': 745,\n",
       "   'most': 746,\n",
       "   '##ream': 747,\n",
       "   '##ince': 748,\n",
       "   'care': 749,\n",
       "   '##ne': 750,\n",
       "   '##iend': 751,\n",
       "   'both': 752,\n",
       "   'gi': 753,\n",
       "   'girl': 754,\n",
       "   'friend': 755,\n",
       "   '##ool': 756,\n",
       "   'always': 757,\n",
       "   'ass': 758,\n",
       "   '##dded': 759,\n",
       "   'next': 760,\n",
       "   '##ons': 761,\n",
       "   '##cked': 762,\n",
       "   '##ner': 763,\n",
       "   'place': 764,\n",
       "   'sil': 765,\n",
       "   'att': 766,\n",
       "   'cur': 767,\n",
       "   'min': 768,\n",
       "   'hold': 769,\n",
       "   '##ower': 770,\n",
       "   'imp': 771,\n",
       "   'oh': 772,\n",
       "   'kill': 773,\n",
       "   'smil': 774,\n",
       "   'house': 775,\n",
       "   'mouth': 776,\n",
       "   'inside': 777,\n",
       "   'sat': 778,\n",
       "   'run': 779,\n",
       "   'found': 780,\n",
       "   'front': 781,\n",
       "   '##row': 782,\n",
       "   'ple': 783,\n",
       "   '##ted': 784,\n",
       "   'heart': 785,\n",
       "   'inst': 786,\n",
       "   'without': 787,\n",
       "   'dec': 788,\n",
       "   'ser': 789,\n",
       "   'gre': 790,\n",
       "   '##less': 791,\n",
       "   '##ating': 792,\n",
       "   'home': 793,\n",
       "   '##ian': 794,\n",
       "   'rel': 795,\n",
       "   'blood': 796,\n",
       "   'same': 797,\n",
       "   '##ory': 798,\n",
       "   'work': 799,\n",
       "   '##aw': 800,\n",
       "   'everything': 801,\n",
       "   'rest': 802,\n",
       "   '##ually': 803,\n",
       "   'someone': 804,\n",
       "   'himself': 805,\n",
       "   'sho': 806,\n",
       "   'stand': 807,\n",
       "   'inter': 808,\n",
       "   'small': 809,\n",
       "   '##ary': 810,\n",
       "   'woman': 811,\n",
       "   'hop': 812,\n",
       "   'bit': 813,\n",
       "   'bre': 814,\n",
       "   '##sw': 815,\n",
       "   'trying': 816,\n",
       "   'heard': 817,\n",
       "   'better': 818,\n",
       "   'ok': 819,\n",
       "   'arms': 820,\n",
       "   'pulled': 821,\n",
       "   'flo': 822,\n",
       "   'cor': 823,\n",
       "   'dra': 824,\n",
       "   'ret': 825,\n",
       "   'cour': 826,\n",
       "   'betw': 827,\n",
       "   'each': 828,\n",
       "   'between': 829,\n",
       "   'father': 830,\n",
       "   'stre': 831,\n",
       "   'give': 832,\n",
       "   '##ised': 833,\n",
       "   'kiss': 834,\n",
       "   'mean': 835,\n",
       "   'wind': 836,\n",
       "   '##cond': 837,\n",
       "   'mother': 838,\n",
       "   'seemed': 839,\n",
       "   'black': 840,\n",
       "   '##ier': 841,\n",
       "   'deep': 842,\n",
       "   '##ung': 843,\n",
       "   'smile': 844,\n",
       "   '##red': 845,\n",
       "   'second': 846,\n",
       "   'answ': 847,\n",
       "   '##pping': 848,\n",
       "   '##if': 849,\n",
       "   '##ange': 850,\n",
       "   '##tered': 851,\n",
       "   'far': 852,\n",
       "   'ang': 853,\n",
       "   'alm': 854,\n",
       "   '##tain': 855,\n",
       "   'across': 856,\n",
       "   '##ably': 857,\n",
       "   '##sp': 858,\n",
       "   'fam': 859,\n",
       "   'pain': 860,\n",
       "   'stop': 861,\n",
       "   'myself': 862,\n",
       "   'hur': 863,\n",
       "   'sto': 864,\n",
       "   'three': 865,\n",
       "   '##ready': 866,\n",
       "   '##sh': 867,\n",
       "   '##used': 868,\n",
       "   'since': 869,\n",
       "   'dri': 870,\n",
       "   'already': 871,\n",
       "   'pass': 872,\n",
       "   '##ever': 873,\n",
       "   'sle': 874,\n",
       "   '##aught': 875,\n",
       "   'started': 876,\n",
       "   'point': 877,\n",
       "   '##ump': 878,\n",
       "   'quick': 879,\n",
       "   'almost': 880,\n",
       "   'gave': 881,\n",
       "   'stu': 882,\n",
       "   'breath': 883,\n",
       "   '##ict': 884,\n",
       "   'dont': 885,\n",
       "   'years': 886,\n",
       "   'must': 887,\n",
       "   'belie': 888,\n",
       "   'stay': 889,\n",
       "   '##ached': 890,\n",
       "   'stood': 891,\n",
       "   'men': 892,\n",
       "   'kind': 893,\n",
       "   '##ix': 894,\n",
       "   '##ank': 895,\n",
       "   'remem': 896,\n",
       "   'ear': 897,\n",
       "   'air': 898,\n",
       "   'else': 899,\n",
       "   'done': 900,\n",
       "   'mor': 901,\n",
       "   'wall': 902,\n",
       "   'dro': 903,\n",
       "   'okay': 904,\n",
       "   'foll': 905,\n",
       "   '##ors': 906,\n",
       "   'doing': 907,\n",
       "   'hell': 908,\n",
       "   '##gether': 909,\n",
       "   'tried': 910,\n",
       "   'tra': 911,\n",
       "   '##umb': 912,\n",
       "   'world': 913,\n",
       "   'ob': 914,\n",
       "   '##ible': 915,\n",
       "   '##ings': 916,\n",
       "   'lips': 917,\n",
       "   'walked': 918,\n",
       "   'close': 919,\n",
       "   '##ane': 920,\n",
       "   'words': 921,\n",
       "   'big': 922,\n",
       "   'ev': 923,\n",
       "   'hum': 924,\n",
       "   'tou': 925,\n",
       "   'mon': 926,\n",
       "   'slow': 927,\n",
       "   '##ater': 928,\n",
       "   'minut': 929,\n",
       "   'together': 930,\n",
       "   '##rew': 931,\n",
       "   '##ized': 932,\n",
       "   'feet': 933,\n",
       "   'nodded': 934,\n",
       "   'ye': 935,\n",
       "   'sig': 936,\n",
       "   '##ott': 937,\n",
       "   '##so': 938,\n",
       "   '##ple': 939,\n",
       "   '##ense': 940,\n",
       "   'didnt': 941,\n",
       "   '##ushed': 942,\n",
       "   'wonder': 943,\n",
       "   'held': 944,\n",
       "   'near': 945,\n",
       "   'seen': 946,\n",
       "   '##ph': 947,\n",
       "   'met': 948,\n",
       "   'many': 949,\n",
       "   'miss': 950,\n",
       "   '##ured': 951,\n",
       "   '##ren': 952,\n",
       "   'these': 953,\n",
       "   'lot': 954,\n",
       "   'ra': 955,\n",
       "   'quest': 956,\n",
       "   'underst': 957,\n",
       "   '##ully': 958,\n",
       "   '##anc': 959,\n",
       "   '##ash': 960,\n",
       "   'bar': 961,\n",
       "   'diff': 962,\n",
       "   '##ently': 963,\n",
       "   'ma': 964,\n",
       "   'supp': 965,\n",
       "   'int': 966,\n",
       "   '##gs': 967,\n",
       "   '##irt': 968,\n",
       "   '##ason': 969,\n",
       "   'tre': 970,\n",
       "   'sim': 971,\n",
       "   'whis': 972,\n",
       "   'pers': 973,\n",
       "   'ent': 974,\n",
       "   '##ond': 975,\n",
       "   '##icked': 976,\n",
       "   '##inking': 977,\n",
       "   'hear': 978,\n",
       "   '##pr': 979,\n",
       "   'play': 980,\n",
       "   'yet': 981,\n",
       "   'talk': 982,\n",
       "   '**': 983,\n",
       "   'says': 984,\n",
       "   'move': 985,\n",
       "   '##ither': 986,\n",
       "   'conf': 987,\n",
       "   'bad': 988,\n",
       "   '##ows': 989,\n",
       "   '##akes': 990,\n",
       "   'floor': 991,\n",
       "   'water': 992,\n",
       "   'expl': 993,\n",
       "   'leave': 994,\n",
       "   'ed': 995,\n",
       "   'smiled': 996,\n",
       "   'mr': 997,\n",
       "   '##ib': 998,\n",
       "   '##day': 999,\n",
       "   ...},\n",
       "  'merges': ['##h ##e',\n",
       "   't ##he',\n",
       "   '##i ##n',\n",
       "   '##e ##r',\n",
       "   '##e ##d',\n",
       "   '##o ##u',\n",
       "   '##n ##d',\n",
       "   '##in ##g',\n",
       "   't ##o',\n",
       "   '##a ##t',\n",
       "   '##r ##e',\n",
       "   't ##h',\n",
       "   '##i ##s',\n",
       "   'a ##nd',\n",
       "   '##a ##s',\n",
       "   '##e ##n',\n",
       "   '##a ##n',\n",
       "   '##o ##n',\n",
       "   '##l ##l',\n",
       "   'h ##e',\n",
       "   '##a ##r',\n",
       "   '##o ##r',\n",
       "   '##e ##s',\n",
       "   'o ##f',\n",
       "   'i ##n',\n",
       "   '##i ##t',\n",
       "   '` ##`',\n",
       "   \"' ##'\",\n",
       "   'h ##a',\n",
       "   '##o ##m',\n",
       "   'y ##ou',\n",
       "   '##o ##w',\n",
       "   'b ##e',\n",
       "   'h ##er',\n",
       "   '##i ##d',\n",
       "   '##u ##t',\n",
       "   'w ##as',\n",
       "   'i ##t',\n",
       "   '##o ##t',\n",
       "   '##l ##e',\n",
       "   '##l ##d',\n",
       "   '##a ##c',\n",
       "   '##g ##h',\n",
       "   'o ##n',\n",
       "   's ##he',\n",
       "   '##e ##t',\n",
       "   's ##t',\n",
       "   'h ##is',\n",
       "   'th ##at',\n",
       "   '##l ##y',\n",
       "   '##i ##m',\n",
       "   '##a ##y',\n",
       "   '##s ##t',\n",
       "   '##v ##e',\n",
       "   '##i ##c',\n",
       "   'w ##h',\n",
       "   'a ##s',\n",
       "   'r ##e',\n",
       "   'm ##y',\n",
       "   'm ##e',\n",
       "   '##a ##d',\n",
       "   '##a ##l',\n",
       "   '##s ##e',\n",
       "   '##o ##o',\n",
       "   '##v ##er',\n",
       "   '##gh ##t',\n",
       "   '##ou ##ld',\n",
       "   '##u ##r',\n",
       "   '##it ##h',\n",
       "   '##i ##r',\n",
       "   'f ##or',\n",
       "   'w ##ith',\n",
       "   '##en ##t',\n",
       "   'd ##o',\n",
       "   'a ##n',\n",
       "   '##k ##e',\n",
       "   'ha ##d',\n",
       "   'l ##i',\n",
       "   'a ##t',\n",
       "   's ##h',\n",
       "   '##a ##m',\n",
       "   'w ##e',\n",
       "   'b ##ut',\n",
       "   '##i ##on',\n",
       "   'h ##im',\n",
       "   '##a ##ll',\n",
       "   'f ##r',\n",
       "   '##he ##r',\n",
       "   's ##e',\n",
       "   's ##a',\n",
       "   'n ##ot',\n",
       "   '##t ##er',\n",
       "   '##i ##ght',\n",
       "   '##k ##ed',\n",
       "   '##p ##p',\n",
       "   'n ##e',\n",
       "   '##r ##o',\n",
       "   '##ac ##k',\n",
       "   's ##o',\n",
       "   '##a ##in',\n",
       "   '##c ##h',\n",
       "   '##i ##ll',\n",
       "   '##an ##t',\n",
       "   '##c ##e',\n",
       "   '##i ##l',\n",
       "   'k ##n',\n",
       "   'o ##ut',\n",
       "   'wh ##at',\n",
       "   '##o ##re',\n",
       "   'i ##s',\n",
       "   '##e ##l',\n",
       "   'u ##p',\n",
       "   '##ou ##t',\n",
       "   '##u ##st',\n",
       "   '##r ##ou',\n",
       "   '##r ##i',\n",
       "   'w ##he',\n",
       "   'ha ##ve',\n",
       "   '##om ##e',\n",
       "   'sa ##id',\n",
       "   'the ##y',\n",
       "   'n ##o',\n",
       "   '##h ##ing',\n",
       "   '##ar ##d',\n",
       "   'th ##is',\n",
       "   'c ##h',\n",
       "   'fr ##om',\n",
       "   'g ##o',\n",
       "   's ##u',\n",
       "   'd ##id',\n",
       "   'a ##b',\n",
       "   '##r ##a',\n",
       "   '##c ##t',\n",
       "   'l ##e',\n",
       "   '##n ##t',\n",
       "   '##k ##ing',\n",
       "   '##er ##e',\n",
       "   'a ##ll',\n",
       "   '##i ##e',\n",
       "   'd ##e',\n",
       "   'a ##l',\n",
       "   '##u ##s',\n",
       "   '##on ##e',\n",
       "   'c ##ould',\n",
       "   '##in ##d',\n",
       "   'l ##oo',\n",
       "   'b ##ack',\n",
       "   'w ##ould',\n",
       "   'w ##ere',\n",
       "   '##t ##o',\n",
       "   '##he ##d',\n",
       "   'li ##ke',\n",
       "   'f ##e',\n",
       "   'i ##f',\n",
       "   '##es ##s',\n",
       "   '##ar ##t',\n",
       "   '##g ##e',\n",
       "   '##ow ##n',\n",
       "   '##es ##t',\n",
       "   'on ##e',\n",
       "   'c ##l',\n",
       "   'the ##re',\n",
       "   '##u ##n',\n",
       "   '##oo ##d',\n",
       "   's ##p',\n",
       "   'c ##on',\n",
       "   '##s ##s',\n",
       "   'j ##ust',\n",
       "   'a ##g',\n",
       "   'in ##to',\n",
       "   '##a ##b',\n",
       "   '##as ##t',\n",
       "   '##a ##u',\n",
       "   'whe ##n',\n",
       "   'w ##or',\n",
       "   'e ##x',\n",
       "   'kn ##ow',\n",
       "   '##er ##s',\n",
       "   '##r ##y',\n",
       "   'r ##o',\n",
       "   '##id ##e',\n",
       "   'ab ##out',\n",
       "   '##e ##p',\n",
       "   '##c ##k',\n",
       "   '##ac ##e',\n",
       "   '##a ##nd',\n",
       "   'b ##l',\n",
       "   'o ##r',\n",
       "   'ha ##nd',\n",
       "   '##all ##y',\n",
       "   '##a ##ke',\n",
       "   'the ##n',\n",
       "   '##u ##m',\n",
       "   'se ##e',\n",
       "   'the ##m',\n",
       "   'you ##r',\n",
       "   'w ##ant',\n",
       "   '##w ##ay',\n",
       "   'o ##ver',\n",
       "   '##u ##l',\n",
       "   't ##e',\n",
       "   'an ##y',\n",
       "   '##o ##p',\n",
       "   'a ##re',\n",
       "   '##on ##g',\n",
       "   '##l ##f',\n",
       "   'e ##y',\n",
       "   'be ##en',\n",
       "   '##is ##t',\n",
       "   '##at ##e',\n",
       "   's ##m',\n",
       "   'g ##et',\n",
       "   '##v ##en',\n",
       "   't ##im',\n",
       "   '##a ##p',\n",
       "   'p ##l',\n",
       "   'd ##own',\n",
       "   'l ##o',\n",
       "   'c ##o',\n",
       "   'e ##ver',\n",
       "   's ##ome',\n",
       "   'm ##o',\n",
       "   'to ##o',\n",
       "   '##i ##g',\n",
       "   '##am ##e',\n",
       "   'b ##o',\n",
       "   'ag ##ain',\n",
       "   '##ur ##n',\n",
       "   '##f ##t',\n",
       "   'th ##ou',\n",
       "   'c ##om',\n",
       "   '##a ##re',\n",
       "   'wh ##o',\n",
       "   '##rou ##nd',\n",
       "   'm ##an',\n",
       "   'of ##f',\n",
       "   '##at ##ion',\n",
       "   '##ic ##e',\n",
       "   'a ##r',\n",
       "   'b ##r',\n",
       "   'tim ##e',\n",
       "   'n ##ow',\n",
       "   'h ##ow',\n",
       "   '##in ##k',\n",
       "   'f ##l',\n",
       "   'b ##y',\n",
       "   '##f ##f',\n",
       "   'p ##o',\n",
       "   '##ou ##s',\n",
       "   'ey ##es',\n",
       "   '##a ##g',\n",
       "   'he ##ad',\n",
       "   'u ##s',\n",
       "   'q ##u',\n",
       "   'm ##ore',\n",
       "   '##ic ##k',\n",
       "   '##i ##ve',\n",
       "   '##or ##t',\n",
       "   '##in ##e',\n",
       "   '##u ##ll',\n",
       "   'c ##an',\n",
       "   's ##l',\n",
       "   'e ##n',\n",
       "   '##a ##k',\n",
       "   's ##c',\n",
       "   't ##r',\n",
       "   '##pp ##ed',\n",
       "   '##e ##ct',\n",
       "   '##t ##her',\n",
       "   '##ot ##her',\n",
       "   't ##w',\n",
       "   '##ou ##nd',\n",
       "   '##o ##s',\n",
       "   'th ##an',\n",
       "   'u ##n',\n",
       "   '##ou ##r',\n",
       "   '##nd ##er',\n",
       "   '##it ##t',\n",
       "   '##se ##lf',\n",
       "   'e ##ven',\n",
       "   'the ##ir',\n",
       "   '##v ##ed',\n",
       "   'w ##ay',\n",
       "   'i ##m',\n",
       "   '##f ##ore',\n",
       "   '##l ##ed',\n",
       "   '##i ##le',\n",
       "   '##d ##d',\n",
       "   'be ##fore',\n",
       "   'g ##r',\n",
       "   '##i ##re',\n",
       "   'ever ##y',\n",
       "   'a ##round',\n",
       "   'w ##ill',\n",
       "   '. ##.',\n",
       "   '##re ##at',\n",
       "   's ##ay',\n",
       "   'go ##ing',\n",
       "   'o ##ther',\n",
       "   '##as ##s',\n",
       "   '.. ##.',\n",
       "   '##u ##re',\n",
       "   'p ##re',\n",
       "   '##v ##ing',\n",
       "   'her ##e',\n",
       "   'p ##ro',\n",
       "   'a ##f',\n",
       "   's ##om',\n",
       "   'r ##ight',\n",
       "   '##t ##h',\n",
       "   '##ad ##e',\n",
       "   'som ##et',\n",
       "   'on ##ly',\n",
       "   'th ##ink',\n",
       "   '##rou ##gh',\n",
       "   'be ##c',\n",
       "   't ##urn',\n",
       "   'th ##rough',\n",
       "   'ne ##ed',\n",
       "   'g ##l',\n",
       "   '##u ##e',\n",
       "   '##i ##p',\n",
       "   'loo ##ked',\n",
       "   '##e ##ar',\n",
       "   '##er ##ed',\n",
       "   '##o ##se',\n",
       "   'loo ##k',\n",
       "   'p ##r',\n",
       "   '##f ##e',\n",
       "   'l ##et',\n",
       "   'thou ##ght',\n",
       "   '##it ##e',\n",
       "   'sh ##ould',\n",
       "   's ##w',\n",
       "   'th ##ing',\n",
       "   '##w ##ard',\n",
       "   '##ac ##t',\n",
       "   '##is ##s',\n",
       "   'st ##ill',\n",
       "   'af ##ter',\n",
       "   'a ##way',\n",
       "   '##o ##l',\n",
       "   'g ##u',\n",
       "   'kn ##e',\n",
       "   'somet ##hing',\n",
       "   'f ##ace',\n",
       "   '##t ##hing',\n",
       "   'a ##m',\n",
       "   'do ##or',\n",
       "   '##p ##t',\n",
       "   'm ##u',\n",
       "   '##on ##t',\n",
       "   'ex ##p',\n",
       "   '##it ##y',\n",
       "   'g ##ood',\n",
       "   '##i ##es',\n",
       "   '##au ##se',\n",
       "   'l ##ong',\n",
       "   '##an ##g',\n",
       "   'ne ##ver',\n",
       "   '##m ##ent',\n",
       "   'w ##at',\n",
       "   'a ##c',\n",
       "   '##en ##ed',\n",
       "   'p ##e',\n",
       "   'w ##a',\n",
       "   '##ac ##h',\n",
       "   '##d ##y',\n",
       "   '##i ##z',\n",
       "   'be ##t',\n",
       "   'fe ##el',\n",
       "   'l ##l',\n",
       "   '##b ##le',\n",
       "   'g ##ot',\n",
       "   'we ##ll',\n",
       "   'as ##ked',\n",
       "   'whe ##re',\n",
       "   'c ##he',\n",
       "   '##ab ##le',\n",
       "   'c ##ar',\n",
       "   '##ar ##k',\n",
       "   'tw ##o',\n",
       "   '##re ##e',\n",
       "   'o ##p',\n",
       "   '##at ##ed',\n",
       "   'f ##o',\n",
       "   '##w ##n',\n",
       "   '##an ##ce',\n",
       "   '##e ##ll',\n",
       "   'f ##ir',\n",
       "   'ar ##m',\n",
       "   'm ##ade',\n",
       "   '##c ##hed',\n",
       "   'wh ##y',\n",
       "   '##ic ##h',\n",
       "   'mu ##ch',\n",
       "   'v ##e',\n",
       "   '##m ##ed',\n",
       "   '##a ##ve',\n",
       "   '##at ##her',\n",
       "   '##i ##ous',\n",
       "   'o ##ur',\n",
       "   'bec ##ause',\n",
       "   'te ##ll',\n",
       "   '##t ##ing',\n",
       "   're ##m',\n",
       "   'ro ##om',\n",
       "   'kne ##w',\n",
       "   'd ##is',\n",
       "   '##itt ##le',\n",
       "   '##a ##ge',\n",
       "   '##a ##king',\n",
       "   '##at ##h',\n",
       "   'u ##nder',\n",
       "   'h ##o',\n",
       "   'l ##ittle',\n",
       "   'm ##om',\n",
       "   '##t ##e',\n",
       "   '##n ##ing',\n",
       "   '##ad ##y',\n",
       "   '##ou ##gh',\n",
       "   'd ##ay',\n",
       "   'c ##r',\n",
       "   'k ##e',\n",
       "   'm ##ake',\n",
       "   '##er ##y',\n",
       "   'c ##ome',\n",
       "   '##c ##o',\n",
       "   't ##ake',\n",
       "   '##el ##t',\n",
       "   '##g ##g',\n",
       "   'c ##all',\n",
       "   'ha ##pp',\n",
       "   'w ##al',\n",
       "   'e ##m',\n",
       "   'm ##ay',\n",
       "   '##ri ##ed',\n",
       "   '##as ##e',\n",
       "   '##f ##ul',\n",
       "   '##k ##s',\n",
       "   'fir ##st',\n",
       "   '##re ##ss',\n",
       "   'con ##t',\n",
       "   '##s ##ide',\n",
       "   '##b ##e',\n",
       "   '##r ##u',\n",
       "   '##a ##ir',\n",
       "   'v ##o',\n",
       "   '##b ##er',\n",
       "   'it ##s',\n",
       "   'f ##elt',\n",
       "   'j ##o',\n",
       "   '##en ##ce',\n",
       "   'su ##re',\n",
       "   'w ##o',\n",
       "   'c ##a',\n",
       "   '##v ##es',\n",
       "   'st ##art',\n",
       "   'hand ##s',\n",
       "   '##i ##ed',\n",
       "   't ##al',\n",
       "   '##an ##s',\n",
       "   '##ar ##ed',\n",
       "   'he ##l',\n",
       "   'p ##ull',\n",
       "   'want ##ed',\n",
       "   'too ##k',\n",
       "   '##a ##h',\n",
       "   'com ##p',\n",
       "   '##in ##t',\n",
       "   'turn ##ed',\n",
       "   '##e ##m',\n",
       "   'n ##ight',\n",
       "   'e ##l',\n",
       "   'a ##pp',\n",
       "   's ##k',\n",
       "   're ##ally',\n",
       "   '##l ##ing',\n",
       "   '##p ##s',\n",
       "   'p ##er',\n",
       "   'd ##es',\n",
       "   'm ##ar',\n",
       "   '##ot ##h',\n",
       "   't ##ry',\n",
       "   '##i ##ld',\n",
       "   '##i ##ly',\n",
       "   'again ##st',\n",
       "   'vo ##ice',\n",
       "   's ##ur',\n",
       "   'h ##as',\n",
       "   'a ##d',\n",
       "   '##u ##ck',\n",
       "   '##i ##a',\n",
       "   'p ##art',\n",
       "   'r ##es',\n",
       "   'be ##l',\n",
       "   '##u ##d',\n",
       "   'f ##in',\n",
       "   'h ##ard',\n",
       "   '##q ##u',\n",
       "   'pe ##op',\n",
       "   're ##al',\n",
       "   '##c ##ed',\n",
       "   'le ##ft',\n",
       "   'wh ##ich',\n",
       "   '##en ##s',\n",
       "   '##b ##b',\n",
       "   'v ##ery',\n",
       "   'be ##h',\n",
       "   'hel ##p',\n",
       "   'c ##ou',\n",
       "   'peop ##le',\n",
       "   'c ##ame',\n",
       "   'to ##ld',\n",
       "   'an ##other',\n",
       "   '##d ##s',\n",
       "   'l ##ast',\n",
       "   'p ##ut',\n",
       "   '##ow ##ed',\n",
       "   '##is ##e',\n",
       "   's ##ide',\n",
       "   'li ##fe',\n",
       "   '##e ##en',\n",
       "   '##oo ##k',\n",
       "   '##is ##h',\n",
       "   'wh ##ile',\n",
       "   '##ro ##ss',\n",
       "   'bo ##dy',\n",
       "   'w ##om',\n",
       "   'wa ##it',\n",
       "   '##t ##y',\n",
       "   'f ##ind',\n",
       "   'fe ##w',\n",
       "   's ##n',\n",
       "   '##a ##v',\n",
       "   'sp ##e',\n",
       "   'g ##ra',\n",
       "   'any ##thing',\n",
       "   're ##p',\n",
       "   'mom ##ent',\n",
       "   'y ##es',\n",
       "   '##a ##z',\n",
       "   'be ##ing',\n",
       "   '##l ##es',\n",
       "   'ne ##w',\n",
       "   '##he ##s',\n",
       "   'w ##ar',\n",
       "   'b ##reat',\n",
       "   'not ##hing',\n",
       "   '##n ##ed',\n",
       "   '##g ##et',\n",
       "   'p ##h',\n",
       "   '##x ##t',\n",
       "   'st ##r',\n",
       "   'o ##wn',\n",
       "   'd ##i',\n",
       "   'beh ##ind',\n",
       "   'st ##e',\n",
       "   'ke ##ep',\n",
       "   'u ##nt',\n",
       "   '##ion ##s',\n",
       "   'y ##ear',\n",
       "   'en ##ough',\n",
       "   'to ##ward',\n",
       "   'w ##ent',\n",
       "   'li ##ght',\n",
       "   'l ##au',\n",
       "   'do ##es',\n",
       "   'm ##at',\n",
       "   'm ##ight',\n",
       "   '##ou ##se',\n",
       "   'sa ##w',\n",
       "   'w ##r',\n",
       "   'ha ##ir',\n",
       "   'b ##ed',\n",
       "   'd ##r',\n",
       "   '##c ##i',\n",
       "   '##i ##gh',\n",
       "   '##p ##er',\n",
       "   'be ##g',\n",
       "   'd ##ark',\n",
       "   'cl ##os',\n",
       "   's ##ound',\n",
       "   'g ##ir',\n",
       "   'o ##ld',\n",
       "   'h ##u',\n",
       "   'unt ##il',\n",
       "   'thing ##s',\n",
       "   'lo ##ve',\n",
       "   'm ##ind',\n",
       "   'c ##le',\n",
       "   '##out ##h',\n",
       "   '##way ##s',\n",
       "   'thou ##gh',\n",
       "   'i ##d',\n",
       "   '##ar ##s',\n",
       "   'may ##be',\n",
       "   'b ##u',\n",
       "   'th ##ose',\n",
       "   's ##et',\n",
       "   '##am ##p',\n",
       "   '##ch ##ing',\n",
       "   '##a ##il',\n",
       "   'op ##en',\n",
       "   'e ##nd',\n",
       "   '##y ##s',\n",
       "   'a ##ct',\n",
       "   '##ar ##ing',\n",
       "   'loo ##king',\n",
       "   'f ##ing',\n",
       "   '##n ##ess',\n",
       "   'on ##ce',\n",
       "   '##n ##g',\n",
       "   '##or ##d',\n",
       "   '##o ##st',\n",
       "   '##or ##m',\n",
       "   'mo ##st',\n",
       "   '##re ##am',\n",
       "   '##in ##ce',\n",
       "   'c ##are',\n",
       "   '##n ##e',\n",
       "   '##ie ##nd',\n",
       "   'b ##oth',\n",
       "   'g ##i',\n",
       "   'gir ##l',\n",
       "   'fr ##iend',\n",
       "   '##oo ##l',\n",
       "   'al ##ways',\n",
       "   'as ##s',\n",
       "   '##dd ##ed',\n",
       "   'ne ##xt',\n",
       "   '##on ##s',\n",
       "   '##c ##ked',\n",
       "   '##n ##er',\n",
       "   'pl ##ace',\n",
       "   's ##il',\n",
       "   'at ##t',\n",
       "   'c ##ur',\n",
       "   'm ##in',\n",
       "   'ho ##ld',\n",
       "   '##ow ##er',\n",
       "   'im ##p',\n",
       "   'o ##h',\n",
       "   'k ##ill',\n",
       "   'sm ##il',\n",
       "   'h ##ouse',\n",
       "   'm ##outh',\n",
       "   'in ##side',\n",
       "   's ##at',\n",
       "   'r ##un',\n",
       "   'f ##ound',\n",
       "   'fr ##ont',\n",
       "   '##r ##ow',\n",
       "   'p ##le',\n",
       "   '##t ##ed',\n",
       "   'he ##art',\n",
       "   'in ##st',\n",
       "   'with ##out',\n",
       "   'de ##c',\n",
       "   's ##er',\n",
       "   'g ##re',\n",
       "   '##l ##ess',\n",
       "   '##at ##ing',\n",
       "   'h ##ome',\n",
       "   '##i ##an',\n",
       "   're ##l',\n",
       "   'bl ##ood',\n",
       "   's ##ame',\n",
       "   '##or ##y',\n",
       "   'wor ##k',\n",
       "   '##a ##w',\n",
       "   'every ##thing',\n",
       "   'r ##est',\n",
       "   '##u ##ally',\n",
       "   'some ##one',\n",
       "   'him ##self',\n",
       "   'sh ##o',\n",
       "   'st ##and',\n",
       "   'in ##ter',\n",
       "   'sm ##all',\n",
       "   '##ar ##y',\n",
       "   'wom ##an',\n",
       "   'h ##op',\n",
       "   'b ##it',\n",
       "   'b ##re',\n",
       "   '##s ##w',\n",
       "   'try ##ing',\n",
       "   'he ##ard',\n",
       "   'bet ##ter',\n",
       "   'o ##k',\n",
       "   'arm ##s',\n",
       "   'pull ##ed',\n",
       "   'fl ##o',\n",
       "   'c ##or',\n",
       "   'd ##ra',\n",
       "   'r ##et',\n",
       "   'c ##our',\n",
       "   'bet ##w',\n",
       "   'e ##ach',\n",
       "   'betw ##een',\n",
       "   'f ##ather',\n",
       "   'st ##re',\n",
       "   'g ##ive',\n",
       "   '##is ##ed',\n",
       "   'k ##iss',\n",
       "   'me ##an',\n",
       "   'w ##ind',\n",
       "   '##co ##nd',\n",
       "   'm ##other',\n",
       "   'see ##med',\n",
       "   'bl ##ack',\n",
       "   '##i ##er',\n",
       "   'de ##ep',\n",
       "   '##un ##g',\n",
       "   'sm ##ile',\n",
       "   '##r ##ed',\n",
       "   'se ##cond',\n",
       "   'an ##sw',\n",
       "   '##pp ##ing',\n",
       "   '##i ##f',\n",
       "   '##an ##ge',\n",
       "   '##ter ##ed',\n",
       "   'f ##ar',\n",
       "   'an ##g',\n",
       "   'al ##m',\n",
       "   '##t ##ain',\n",
       "   'ac ##ross',\n",
       "   '##ab ##ly',\n",
       "   '##s ##p',\n",
       "   'f ##am',\n",
       "   'p ##ain',\n",
       "   'st ##op',\n",
       "   'my ##self',\n",
       "   'h ##ur',\n",
       "   'st ##o',\n",
       "   'th ##ree',\n",
       "   '##re ##ady',\n",
       "   '##s ##h',\n",
       "   '##us ##ed',\n",
       "   's ##ince',\n",
       "   'd ##ri',\n",
       "   'al ##ready',\n",
       "   'p ##ass',\n",
       "   '##e ##ver',\n",
       "   's ##le',\n",
       "   '##au ##ght',\n",
       "   'start ##ed',\n",
       "   'po ##int',\n",
       "   '##um ##p',\n",
       "   'qu ##ick',\n",
       "   'alm ##ost',\n",
       "   'g ##ave',\n",
       "   'st ##u',\n",
       "   'breat ##h',\n",
       "   '##ic ##t',\n",
       "   'd ##ont',\n",
       "   'year ##s',\n",
       "   'm ##ust',\n",
       "   'bel ##ie',\n",
       "   'st ##ay',\n",
       "   '##ac ##hed',\n",
       "   'st ##ood',\n",
       "   'm ##en',\n",
       "   'k ##ind',\n",
       "   '##i ##x',\n",
       "   '##an ##k',\n",
       "   'rem ##em',\n",
       "   'e ##ar',\n",
       "   'a ##ir',\n",
       "   'el ##se',\n",
       "   'd ##one',\n",
       "   'm ##or',\n",
       "   'w ##all',\n",
       "   'd ##ro',\n",
       "   'ok ##ay',\n",
       "   'fo ##ll',\n",
       "   '##or ##s',\n",
       "   'do ##ing',\n",
       "   'he ##ll',\n",
       "   '##get ##her',\n",
       "   't ##ried',\n",
       "   't ##ra',\n",
       "   '##um ##b',\n",
       "   'wor ##ld',\n",
       "   'o ##b',\n",
       "   '##i ##ble',\n",
       "   '##ing ##s',\n",
       "   'li ##ps',\n",
       "   'wal ##ked',\n",
       "   'cl ##ose',\n",
       "   '##an ##e',\n",
       "   'wor ##ds',\n",
       "   'b ##ig',\n",
       "   'e ##v',\n",
       "   'h ##um',\n",
       "   't ##ou',\n",
       "   'm ##on',\n",
       "   'sl ##ow',\n",
       "   '##at ##er',\n",
       "   'min ##ut',\n",
       "   'to ##gether',\n",
       "   '##re ##w',\n",
       "   '##iz ##ed',\n",
       "   'fe ##et',\n",
       "   'no ##dded',\n",
       "   'y ##e',\n",
       "   's ##ig',\n",
       "   '##ot ##t',\n",
       "   '##s ##o',\n",
       "   '##p ##le',\n",
       "   '##en ##se',\n",
       "   'did ##nt',\n",
       "   '##us ##hed',\n",
       "   'wo ##nder',\n",
       "   'he ##ld',\n",
       "   'ne ##ar',\n",
       "   'se ##en',\n",
       "   '##p ##h',\n",
       "   'm ##et',\n",
       "   'man ##y',\n",
       "   'm ##iss',\n",
       "   '##ur ##ed',\n",
       "   '##re ##n',\n",
       "   'the ##se',\n",
       "   'l ##ot',\n",
       "   'r ##a',\n",
       "   'qu ##est',\n",
       "   'under ##st',\n",
       "   '##ull ##y',\n",
       "   '##an ##c',\n",
       "   '##as ##h',\n",
       "   'b ##ar',\n",
       "   'di ##ff',\n",
       "   '##ent ##ly',\n",
       "   'm ##a',\n",
       "   'su ##pp',\n",
       "   'in ##t',\n",
       "   '##g ##s',\n",
       "   '##ir ##t',\n",
       "   '##as ##on',\n",
       "   't ##re',\n",
       "   's ##im',\n",
       "   'wh ##is',\n",
       "   'p ##ers',\n",
       "   'e ##nt',\n",
       "   '##o ##nd',\n",
       "   '##ic ##ked',\n",
       "   '##in ##king',\n",
       "   'he ##ar',\n",
       "   '##p ##r',\n",
       "   'pl ##ay',\n",
       "   'y ##et',\n",
       "   'tal ##k',\n",
       "   '* ##*',\n",
       "   'say ##s',\n",
       "   'mo ##ve',\n",
       "   '##it ##her',\n",
       "   'con ##f',\n",
       "   'b ##ad',\n",
       "   '##ow ##s',\n",
       "   '##ak ##es',\n",
       "   'flo ##or',\n",
       "   'wat ##er',\n",
       "   'exp ##l',\n",
       "   'le ##ave',\n",
       "   'e ##d',\n",
       "   'smil ##ed',\n",
       "   'm ##r',\n",
       "   '##i ##b',\n",
       "   '##d ##ay',\n",
       "   's ##or',\n",
       "   'a ##w',\n",
       "   'r ##ed',\n",
       "   '##i ##v',\n",
       "   '##ain ##ed',\n",
       "   '##g ##n',\n",
       "   'in ##c',\n",
       "   'g ##reat',\n",
       "   'd ##am',\n",
       "   'g ##one',\n",
       "   'st ##ep',\n",
       "   '##gg ##ed',\n",
       "   '##ow ##ing',\n",
       "   't ##able',\n",
       "   'her ##self',\n",
       "   '##y ##ing',\n",
       "   '##u ##b',\n",
       "   'ac ##c',\n",
       "   'e ##as',\n",
       "   '##e ##k',\n",
       "   'fing ##ers',\n",
       "   '##i ##et',\n",
       "   'p ##ast',\n",
       "   'che ##st',\n",
       "   'ch ##ar',\n",
       "   'h ##our',\n",
       "   'need ##ed',\n",
       "   '##ent ##ion',\n",
       "   'pro ##b',\n",
       "   '##d ##er',\n",
       "   '##i ##ver',\n",
       "   'beg ##an',\n",
       "   'd ##ist',\n",
       "   '##n ##y',\n",
       "   '##he ##re',\n",
       "   '##is ##hed',\n",
       "   '##ac ##ed',\n",
       "   'n ##ame',\n",
       "   '##i ##ft',\n",
       "   'al ##so',\n",
       "   'con ##s',\n",
       "   '##ect ##ed',\n",
       "   'sw ##e',\n",
       "   'get ##ting',\n",
       "   'sh ##ook',\n",
       "   'l ##u',\n",
       "   'ha ##lf',\n",
       "   '##ep ##t',\n",
       "   'call ##ed',\n",
       "   '##ri ##e',\n",
       "   'al ##ong',\n",
       "   'ye ##ah',\n",
       "   '##ce ##pt',\n",
       "   'b ##ro',\n",
       "   '##ate ##ly',\n",
       "   't ##er',\n",
       "   'so ##on',\n",
       "   'f ##ore',\n",
       "   'cour ##se',\n",
       "   'b ##en',\n",
       "   'so ##ft',\n",
       "   'com ##ing',\n",
       "   'b ##us',\n",
       "   'fin ##ally',\n",
       "   '##dd ##en',\n",
       "   'f ##all',\n",
       "   'prob ##ably',\n",
       "   'id ##e',\n",
       "   'de ##ad',\n",
       "   'ch ##ild',\n",
       "   'p ##a',\n",
       "   '##ic ##al',\n",
       "   'f ##re',\n",
       "   'su ##ch',\n",
       "   'su ##dden',\n",
       "   'com ##m',\n",
       "   'in ##v',\n",
       "   '##in ##ed',\n",
       "   'go ##d',\n",
       "   'le ##an',\n",
       "   'le ##ast',\n",
       "   '##b ##o',\n",
       "   'b ##est',\n",
       "   'happ ##ened',\n",
       "   'c ##ra',\n",
       "   'mo ##ved',\n",
       "   'st ##ra',\n",
       "   'p ##at',\n",
       "   'h ##igh',\n",
       "   'co ##l',\n",
       "   'po ##ss',\n",
       "   'cont ##in',\n",
       "   'ret ##urn',\n",
       "   '##ct ##ion',\n",
       "   'pers ##on',\n",
       "   '##er ##ing',\n",
       "   'ey ##e',\n",
       "   '##us ##h',\n",
       "   're ##ached',\n",
       "   'sor ##ry',\n",
       "   'belie ##ve',\n",
       "   'res ##p',\n",
       "   'sk ##in',\n",
       "   '##m ##s',\n",
       "   'wh ##ite',\n",
       "   'l ##ater',\n",
       "   'ph ##one',\n",
       "   'f ##ire',\n",
       "   'sto ##pped',\n",
       "   'g ##az',\n",
       "   '##n ##a',\n",
       "   '##bb ##ed',\n",
       "   'bo ##y',\n",
       "   't ##ight',\n",
       "   'l ##ist',\n",
       "   'm ##aking',\n",
       "   'feel ##ing',\n",
       "   '##id ##d',\n",
       "   'de ##m',\n",
       "   'j ##ack',\n",
       "   'sur ##pr',\n",
       "   '##i ##pped',\n",
       "   'as ##k',\n",
       "   '##it ##ion',\n",
       "   'op ##ened',\n",
       "   'pre ##ss',\n",
       "   'a ##p',\n",
       "   'me ##et',\n",
       "   'any ##one',\n",
       "   '##g ##ed',\n",
       "   ...]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hopper.json', 'r') as fin:\n",
    "    data = json.load(fin)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cbb9ddd-7a9a-4110-b185-3ad389741683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'this', 'is', 'so', 'simple', 'to', 'do', 'in', 'h', '##f', '[UNK]', '[UNK]', '##.']\n"
     ]
    }
   ],
   "source": [
    "# remake the tokenizer\n",
    "t2 = Tokenizer.from_file('hopper.json')\n",
    "enc = t2.encode(text)\n",
    "print(enc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e55e6ae0-2b09-45a1-8b57-c6998691faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "bert_tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "bert_tokenizer.normalizer = Lowercase()\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_trainer = BpeTrainer(vocab_size=32_000, special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'], continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be0bab54-4851-4ad9-b1c3-df5ee8a7b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e86f40e-1a95-4097-89af-b20f4af3d1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $0 [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1\",\n",
    "    special_tokens=[(\"[CLS]\", 2), (\"[SEP]\", 3)]\n",
    ")\n",
    "bert_tokenizer.train_from_iterator(get_examples(batch_size=10_000),  trainer=bert_trainer, length=len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a65f98ef-75d4-4d9f-86de-d25e692edfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23b8461c-bbae-4182-a3f7-a552fa858276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 102, 24, 462, 17, 67,\n",
      "         206, 387, 3],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]']}\n"
     ]
    }
   ],
   "source": [
    "text = \"All these are so simple to do in HF. Let's do more\"\n",
    "enc = bert_tokenizer.encode(text)\n",
    "pprint({'ids': enc.ids, 'tokens': enc.tokens}, depth=2, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b61f66fc-8f63-40e1-8cd7-60a056051c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [2, 270, 956, 336, 231, 2534, 141, 206, 157, 56, 102, 24, 462, 17, 67,\n",
      "         206, 387, 3, 214, 250, 49, 490, 415, 141, 260, 12],\n",
      " 'tokens': ['[CLS]', 'all', 'these', 'are', 'so', 'simple', 'to', 'do', 'in',\n",
      "            'h', '##f', '.', 'let', \"'\", 's', 'do', 'more', '[SEP]', 'we',\n",
      "            'have', 'a', 'long', 'way', 'to', 'go', '!']}\n"
     ]
    }
   ],
   "source": [
    "pair = [\n",
    "    text, \"We have a long way to go!\"\n",
    "]\n",
    "enc = bert_tokenizer.encode(*pair)\n",
    "pprint({'ids': enc.ids, 'tokens': enc.tokens}, depth=2, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "336fab35-7741-4d55-9ef3-b5c8c78555db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all these are so simple to do in h ##f . let ' s do more we have a long way to go !\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decoding\n",
    "bert_tokenizer.decode(enc.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b3fee70-682c-4242-b955-32b3c2b4bbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"all these are so simple to do in hf. let ' s do more we have a long way to go!\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.decoders import WordPiece\n",
    "bert_tokenizer.decoder = WordPiece(prefix='##')\n",
    "\n",
    "bert_tokenizer.decode(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95a8cc-0b85-47ec-a7f7-a60798927d0e",
   "metadata": {},
   "source": [
    "## Pretrained Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5122ed7f-c7f4-4b8b-ba8b-03fb6463e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a64ecbfd-d9e7-4138-928c-3062baa31727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2165: FutureWarning: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pt_tokenizer = PreTrainedTokenizerFast.from_pretrained('hopper.json', unk_token='[UNK]', pad_token='[PAD]', model_input_names=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04d52c0c-b0eb-4925-813e-b658561c3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 92, 21, 459, 14, 64,\n",
      "               203, 384],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "pprint(pt_tokenizer(text), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be4132d5-74b5-450c-8342-cedef32ad79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1],\n",
      " 'input_ids': [267, 953, 333, 228, 2531, 138, 203, 154, 53, 92, 21, 459, 14, 64,\n",
      "               203, 384, 211, 247, 46, 487, 412, 138, 257, 9],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "pprint(pt_tokenizer(text, text_pair=pair[-1]), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fd018f5-517b-491b-9371-ce005380dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760],\n",
      "               [772, 9, 1767, 200, 254]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "batch = ['I like the book The Psychology of Money', 'I enjoyed watching the Transformers movie', 'oh! thanks for this']\n",
    "enc = pt_tokenizer(batch)\n",
    "pprint(enc, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ec90c40-61bf-441f-b09c-ae2cae1c25fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "                    [1, 1, 1, 1, 1, 0, 0, 0]],\n",
      " 'input_ids': [[54, 281, 131, 1701, 131, 19478, 153, 1564],\n",
      "               [54, 4096, 1443, 131, 7744, 307, 3760, 0],\n",
      "               [772, 9, 1767, 200, 254, 0, 0, 0]],\n",
      " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                    [0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "enc = pt_tokenizer(batch, padding=True)\n",
    "pprint(enc, compact=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
