{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab55f79-8d46-49c2-87ab-0509136d4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers as trx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f6fd5e-ab96-482e-a49b-a7f69da81fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('yelp/yelp_review_full')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2ff5ac-1a1e-4869-b2bb-87d769cb9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trx.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "cfg = trx.BertConfig()\n",
    "model = trx.BertForMaskedLM(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a278b26-6853-4a7e-8547-ef40d0d81319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109514298"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_params\n",
    "n_params = 0\n",
    "for p in model.parameters():\n",
    "    n_params += p.numel()\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39f2cc6-ea4a-4eea-b607-e4b58f90cceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109.514298"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664dcb2b-7883-4981-bbb0-cb4c6a5d443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109907514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_position_embeddings = 1024\n",
    "long_model = trx.BertForMaskedLM(cfg)\n",
    "n_params_long = 0\n",
    "for p in long_model.parameters():\n",
    "    n_params_long += p.numel()\n",
    "n_params_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f40f4d-b2e4-46df-add0-db036938322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.393216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_params_long - n_params) / 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b98f3-15aa-4e4a-ae90-c7aea359c241",
   "metadata": {},
   "source": [
    "\n",
    " Pack (chunk) the samples such that the length of all the samples in the dataset is 512 (for efficient training). Define a mapping function that implements the following procedure\n",
    " 1. Take a batch of 1000 samples\n",
    " 2. Tokenize it to get input IDs and attention mask\n",
    " 3. Concatenate all the input IDs\n",
    " 4. Chunk the concatenated IDs into a size of 512\n",
    " 5. Drop the last chunk if its length is less than 512\n",
    " 6. Pack all the chunks\n",
    " 7. Iterate over all the batches in the dataset \n",
    "Store the resulting dataset in the variable “ds_chunked”. Enter the total number of samples in the new dataset.\n",
    "Note: the batch size should be kept at 1000 while calling \"ds.map()\" for theanswer to match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1b8fa5-dead-4913-9405-123b3e928700",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('yelp/yelp_review_full', split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4204dc05-79cc-4d41-8e1c-4a6a712e3126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a5cfc19f1545e98f36c3c66a31fd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/700000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(lambda x: {'n_tokens': len(tokenizer(x['text'])['input_ids'])}, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c71603-cfeb-4874-a4c2-2039440ec65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180.70404142857143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ds['n_tokens']) / len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45eb763e-c5d5-4c48-ab58-8d0e24453e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18f53e32-d6ec-4d4f-b7a6-82e8950e7409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb45be24226463ca097570654fc1371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/700000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 246703\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk(batch):\n",
    "    enc = tokenizer(batch['text'])\n",
    "    input_ids = list(chain(*enc['input_ids']))\n",
    "    attention_mask = list(chain(*enc['attention_mask']))\n",
    "    assert len(input_ids) == len(attention_mask)\n",
    "    input_ids_chunked = [input_ids[i: (i + 512)] for i in range(0, len(input_ids), 512)]\n",
    "    attention_mask_chunked = [attention_mask[i: (i + 512)] for i in range(0, len(attention_mask), 512)]\n",
    "    if len(input_ids_chunked[-1]) < 512:\n",
    "        input_ids_chunked = input_ids_chunked[:-1]\n",
    "        attention_mask_chunked = attention_mask_chunked[:-1]\n",
    "    return {'input_ids': input_ids_chunked, 'attention_mask': attention_mask_chunked}\n",
    "\n",
    "ds_chunked = ds.map(chunk, batched=True, batch_size=1000, remove_columns=ds.column_names, num_proc=12)\n",
    "ds_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e476eadd-f0c2-45ff-a34b-25b780458b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a86387531a42a7a078a5c00956fc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/246703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{512}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_chunked = ds_chunked.map(lambda x: {'n_tokens': len(x['input_ids'])}, num_proc=12)\n",
    "set(ds_chunked['n_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf5d402-4d5b-4f1b-bc80-89c3e68c4b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8374198935562194"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds) / len(ds_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c2f201-e160-412b-b231-1d3b23c8b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is good! ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1c25beb-1e7b-4765-b534-7d972ba8bd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'n_tokens'],\n",
       "        num_rows: 234367\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'n_tokens'],\n",
       "        num_rows: 12336\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split = ds_chunked.train_test_split(test_size=0.05, seed=42)\n",
    "ds_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87d681cc-d5b8-4b69-b761-ff0be3129571",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = trx.DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30eaaad6-0c79-4705-9b26-d5a319ab938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82fb3296-8259-44a3-93ff-c8731b809cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1997,  6881,  2073,  ...,  2795,  7020,  2080],\n",
       "        [ 3504,  3651,  1012,  ..., 22640,  2696,  2001],\n",
       "        [ 3403,  2051,   103,  ...,   103,  1056,  3710],\n",
       "        [ 5587, 25861,  5167,  ...,  2000,  2256,   103]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'n_tokens': tensor([512, 512, 512, 512]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, 5236, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, 2003,  ..., 1005, -100, -100],\n",
       "        [-100, 3176, -100,  ..., -100, -100, 2795]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(ds_split['train'], batch_size=4, collate_fn=collator)\n",
    "for batch in loader:\n",
    "    break\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17db93c4-48ea-4b91-a275-f281f3b33eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8047)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch['labels'][0] == -100).sum() / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd6d7b8e-b4fd-4ef0-8c6e-6ddfd9c0d7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d72d70-8f8f-4987-a06e-a9affebdd217",
   "metadata": {},
   "source": [
    "Create a small BERT model by changing the following hyper-parameters and keeping the other hyper-parameters as is\n",
    "\n",
    " * num_hidden_layers = 6\n",
    " * hidden size: 384\n",
    " * intermediate_size: 1536\n",
    " \n",
    " and start training the model with a batch of size 8 for an epoch. What is the loss value at the end of the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74ea3fe0-86e9-4df3-9d09-00c1092b19fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78c97a20-9219-4a95-b6c2-04cdae9d60f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29296' max='29296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29296/29296 2:12:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.328219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.172441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.103881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.062629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.350300</td>\n",
       "      <td>6.024490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.350300</td>\n",
       "      <td>5.993413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.350300</td>\n",
       "      <td>5.979392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>6.350300</td>\n",
       "      <td>5.957824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>6.350300</td>\n",
       "      <td>5.947351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.995200</td>\n",
       "      <td>5.936842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.995200</td>\n",
       "      <td>5.923951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.995200</td>\n",
       "      <td>5.912499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.995200</td>\n",
       "      <td>5.902859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.995200</td>\n",
       "      <td>5.905339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.930700</td>\n",
       "      <td>5.893688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.930700</td>\n",
       "      <td>5.887589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.930700</td>\n",
       "      <td>5.880388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.930700</td>\n",
       "      <td>5.877148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>5.930700</td>\n",
       "      <td>5.870665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.893000</td>\n",
       "      <td>5.862994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>5.893000</td>\n",
       "      <td>5.861741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>5.893000</td>\n",
       "      <td>5.862720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>5.893000</td>\n",
       "      <td>5.859246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>5.893000</td>\n",
       "      <td>5.853010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>5.871800</td>\n",
       "      <td>5.850671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>5.871800</td>\n",
       "      <td>5.850032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>5.871800</td>\n",
       "      <td>5.849793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>5.871800</td>\n",
       "      <td>5.847090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>5.871800</td>\n",
       "      <td>5.848452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = trx.BertConfig()\n",
    "cfg.num_hidden_layers = 6\n",
    "cfg.hidden_size = 384\n",
    "cfg.intermediate_size = 1536\n",
    "model = trx.BertForMaskedLM(cfg)\n",
    "\n",
    "training_args = trx.TrainingArguments(\n",
    "    output_dir=\"out-bert-2\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    tf32=False,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5000,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=10,\n",
    ")\n",
    "trainer = trx.Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=ds_split['train'], eval_dataset=ds_split['test'], data_collator=collator\n",
    ")\n",
    "yuri = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3db1a5a8-eabf-4521-8f16-caf1da9c7d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 7942.4873,\n",
       " 'train_samples_per_second': 29.508,\n",
       " 'train_steps_per_second': 3.689,\n",
       " 'total_flos': 7794944963463168.0,\n",
       " 'train_loss': 5.986819539676918,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yuri.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0ede424-05d0-41c0-a1b4-d5f83c188bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30837.875"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "246703 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c3925-0181-485e-b96e-275ef09cf6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
