{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d4f468-8198-4bd0-949f-90f06bfe88d8",
   "metadata": {},
   "source": [
    "Download the Train splits of datasets \"aalksii/ml-arxiv-papers\" and “Salesforce/wikitext” (the smaller 'wikitext-2-raw-v1' version)\n",
    "\n",
    "● Utilise only the “abstract” column of the arxiv dataset.\n",
    "\n",
    "● Perform appropriate steps to join both the datasets by concatenating them together\n",
    "\n",
    "● Reference versions of packages for this exam:\n",
    "\n",
    "\t○ Transformer version: 4.44.2\n",
    "\t○ Datasets version: 3.1.0\n",
    "\t○ Tokenizers version: 0.19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f664f00-e523-4fbf-a5a1-0b94f48ba0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.44.2\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets==3.1.0\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tokenizers==0.19.1\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from transformers==4.44.2) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from datasets==3.1.0) (3.10.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from aiohttp->datasets==3.1.0) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from requests->transformers==4.44.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from requests->transformers==4.44.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from requests->transformers==4.44.2) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from pandas->datasets==3.1.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, datasets\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.1\n",
      "    Uninstalling tokenizers-0.20.1:\n",
      "      Successfully uninstalled tokenizers-0.20.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.1\n",
      "    Uninstalling transformers-4.46.1:\n",
      "      Successfully uninstalled transformers-4.46.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.21.0\n",
      "    Uninstalling datasets-2.21.0:\n",
      "      Successfully uninstalled datasets-2.21.0\n",
      "Successfully installed datasets-3.1.0 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.44.2\" \"datasets==3.1.0\" \"tokenizers==0.19.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3bdf11-1d3f-4db0-b2d8-1afe98f2fd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfomers version:  4.44.2\n",
      "datasets version:  3.1.0\n",
      "tokenizers version:  0.19.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import tokenizers\n",
    "\n",
    "print('transfomers version: ', transformers.__version__)\n",
    "print('datasets version: ', datasets.__version__)\n",
    "print('tokenizers version: ', tokenizers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f046c06-2831-4d97-a1fe-9b97f21230b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833567b3daf641759bd5ff38c346dec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a631647eea4c4e56affdaeab61601978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-52427cf3bce60f12.parquet:   0%|          | 0.00/73.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575bcd0b5ce48bbb5bbe14296040567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-c5f66ae2f59807ae.parquet:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4964b8879b6344d0bb6bc53a5f49c024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/105832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9236fe9ff59c40e58e9b2ede7e3a5fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract'],\n",
       "    num_rows: 105832\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "arxiv = load_dataset('aalksii/ml-arxiv-papers', split='train')\n",
    "arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6b1491-c862-4259-8e35-58a666f405b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084b2b7787d44d998ad0232206c9542c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b978fc52ad8b4d3cbd3eb4f46a230a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c6f530d2c140228fefc27693b72523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9e782184834b22931b7d76a44f3f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beeb863c8254fad9a0b171075b30279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16d7df4d16646a7b68fcb3ecb6df161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbb5b2f0360455b800dd2245bd738bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext = load_dataset('Salesforce/wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226cfec0-8592-4fb4-9414-3edbd5300e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 105832\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Arxiv dataset\n",
    "arxiv = arxiv.remove_columns('title')\n",
    "arxiv = arxiv.rename_column('abstract', 'text')\n",
    "print(arxiv)\n",
    "print(wikitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8593349d-3cb4-4c44-bb00-c3e54050b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 142550\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "ds = concatenate_datasets([arxiv, wikitext])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4824d76-89d4-4f8f-b38d-eca9e7b200b2",
   "metadata": {},
   "source": [
    "### Data prep done, now problems:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449f75c-f0fa-4896-a2f7-a60e9bc3b475",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. What is the size of combined dataset, in terms of “thousands of number of\n",
    "rows”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1aa6850-32a7-4b8a-9dce-77d4950556e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664b4957-42d7-4857-9395-548cff626742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.55"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.num_rows / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c752e-6c20-490a-9b22-810ee7734537",
   "metadata": {},
   "source": [
    "2. What is the average length of sentences, assuming text in each row is split by a single space split(“ ”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427c4cca-6456-4b10-ae52-8feb11f0b885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330e694814784ff393dc504c1e7cd8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/142550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda x: {'n_tokens': len(x['text'].split(\" \"))}, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce14ef06-ba7c-4334-9b6e-645e1df89100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.88237811294283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ds['n_tokens']) / len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "094b545b-d779-4e83-857a-a5c0eb86dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.88237811294283\n"
     ]
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "for sample in ds:\n",
    "    total_tokens += sample['n_tokens']\n",
    "print(total_tokens / len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65228a16-017a-4480-807a-7075f83d3e6d",
   "metadata": {},
   "source": [
    "3. How many rows have more than or equal to 150 words but less than or equal to 400 words (answer in thousands of number of rows)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d5e1785-8808-4769-b8e6-b4ab207f5633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a320a845ee4096b52604ca9c20b03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/142550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188af470dbe54e28882fc069600a04da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/70581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'n_tokens'],\n",
       "    num_rows: 70519\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = ds.filter(lambda x: x['n_tokens'] >= 150).filter(lambda x: x['n_tokens'] <= 400)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200d640-0db6-4f53-b6f2-21d3a4bc8e8c",
   "metadata": {},
   "source": [
    "4. Now, utilise the BertNormalizer and BertPreTokenizer\n",
    "\n",
    "Answer the following based on the above mentioned steps.\n",
    "\n",
    "Which of the following statements is True?\n",
    "\n",
    "- [ ] BertNormalizer will not change to lowercase by default\n",
    "- [ ] BertNormalizer by default takes care of accented characters.\n",
    "- [ ] BertPreTokenizer will split words only on white spaces while ignoring punctuations\n",
    "- [ ] BertPreTokenizer will split words on white spaces as well as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "968de734-784e-497f-a807-9793b001255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d461377b-0f82-4af8-bccf-ff35dc511d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mBertNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclean_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhandle_chinese_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "BertNormalizer\n",
       "\n",
       "Takes care of normalizing raw text before giving it to a Bert model.\n",
       "This includes cleaning the text, handling accents, chinese chars and lowercasing\n",
       "\n",
       "Args:\n",
       "    clean_text (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether to clean the text, by removing any control characters\n",
       "        and replacing all whitespaces by the classic one.\n",
       "\n",
       "    handle_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether to handle chinese chars by putting spaces around them.\n",
       "\n",
       "    strip_accents (:obj:`bool`, `optional`):\n",
       "        Whether to strip all accents. If this option is not specified (ie == None),\n",
       "        then it will be determined by the value for `lowercase` (as in the original Bert).\n",
       "\n",
       "    lowercase (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether to lowercase.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/envs/dlp/lib/python3.12/site-packages/tokenizers/normalizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BertNormalizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1225bd3e-ff2b-4b39-9fab-b3b9c9077963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mBertPreTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "BertPreTokenizer\n",
       "\n",
       "This pre-tokenizer splits tokens on spaces, and also on punctuation.\n",
       "Each occurence of a punctuation character will be treated separately.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/envs/dlp/lib/python3.12/site-packages/tokenizers/pre_tokenizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BertPreTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7db2baac-47a8-4454-bf29-a03ea21d3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third option false, fourth true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8a1a2-21ba-482b-afee-48b190fd4fb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4. Consider the following text sentence to be encoded:\n",
    "\n",
    "\t○ From misfiring superstars Rohit and Kohli, to New Zealand’s spin attack making the most of the home pitches, here’s how India were handed a shock series loss.\n",
    "\n",
    "● Suppose now you use the WordPiece model, along with its appropriate trainer.\n",
    "● Train two Tokenizer models, of varying Vocabulary sizes: 5000 and 10000\n",
    "● Answer the following based on the above mentioned Normalizer, Tokenize\n",
    "\n",
    "Which of the following statements is True\n",
    "\n",
    "- [ ] The Tokenizer model trained on 5k vocabulary size encodes the given text sentence in fewer tokens.\n",
    "- [ ] The Tokenizer model trained on 10k vocabulary size encodes the given text sentence in fewer tokens.\n",
    "- [ ] The Tokenizer model trained on 10k vocabulary size encodes the given text sentence in half the number of tokens\n",
    "- [ ] Difference in the number of tokens in both the model’s encoding is less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df32915-1116-4912-8ab3-2594c04e738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d1f79bb-ffa4-4318-b8b9-1b4d2284f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd87d607-8c42-4530-afd8-9df9d79a6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8ce0d9a-0fc9-4c9d-b054-49e9d2ad8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5k = WordPiece()\n",
    "model_10k = WordPiece()\n",
    "\n",
    "tok_5k = Tokenizer(model_5k)\n",
    "tok_10k = Tokenizer(model_10k)\n",
    "\n",
    "tok_5k.normalizer = BertNormalizer()\n",
    "tok_10k.normalizer = BertNormalizer()\n",
    "\n",
    "tok_5k.pre_tokenizer = BertPreTokenizer()\n",
    "tok_10k.pre_tokenizer = BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ebcc0db-0aeb-4e50-aea2-99179ecf75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_5k = WordPieceTrainer(vocab_size=5000)\n",
    "trainer_10k = WordPieceTrainer(vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f8b1fc0-ed44-4d0e-afe3-63ecf0857104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we supposed to train on filtered data or on the original concatenated dataset? Let's try both.\n",
    "\n",
    "def iterator_train():\n",
    "    for sample in ds:\n",
    "        yield sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cd58aaf-512a-4734-b07a-80d84d88319b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok_5k.train_from_iterator(iterator_train(), trainer=trainer_5k, length=len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00366eac-0a84-42f0-ba76-a1c20a536af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok_10k.train_from_iterator(iterator_train(), trainer=trainer_10k, length=len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a6993c3-91a0-49c3-b173-2616af650f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"From misfiring superstars Rohit and Kohli, to New Zealand’s spin attack making the most of the home pitches, here’s how India were handed a shock series loss.\"\n",
    "enc_5k = tok_5k.encode(sample)\n",
    "enc_10k = tok_10k.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd0745e1-ce30-41c4-bc7c-5213ceb11209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=53, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a781a623-e274-4206-b2a2-33f8b2f8598b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=48, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd38c59-b932-4b0a-aacf-fbb4f085a9b8",
   "metadata": {},
   "source": [
    "6. Select the true statements for WordPiece Tokenizer\n",
    "\n",
    "- [ ] It is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules.\n",
    "- [ ] It is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it.\n",
    "- [ ] It will tokenize by looking for the most likely segmentation into tokens, according to the model.\n",
    "- [ ] It will tokenize by looking for the longest subword starting from the beginning that is in the vocabulary, then repeat the process for the rest of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849f351-8075-449d-9026-996424919830",
   "metadata": {},
   "source": [
    "From the [docs](https://huggingface.co/docs/transformers/main/en/tokenizer_summary#wordpiece): \n",
    "\n",
    "> WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules.\n",
    "> In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n",
    "\n",
    "Thus, options 2 and 3 are correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b714d8-5160-4f44-a7a0-29cf6ffda541",
   "metadata": {},
   "source": [
    "7 . \n",
    "\n",
    "\n",
    "● Suppose now you retain only those rows in your dataset that consist of greater than or equal to 150 words but fewer than or equal to 400 words, called Filtered_data.\n",
    "\n",
    "● Download the \"bert-base-uncased\" pretrained auto-tokenizer, and tokenize the Filtered_data, and split it into train and test, where test size is 1% of the dataset.\n",
    "\n",
    "● Use appropriate DataCollator and DataLoader (from torch) over the train_split of the above tokenized data and to create data batches of size 100 .\n",
    "\n",
    "● Suppose you use DistilBertConfig, DistilBertForMaskedLM for further tasks\n",
    "\n",
    "How many batches of data exist in Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc13087c-9ba7-40c5-9969-4e16deea83c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "bbu = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b4ee2b1a-0249-40dc-8fdd-746c7d29374d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfc681cbc474acdbd6860fd823de89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/70519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _tokenize(sample):\n",
    "    enc = bbu(sample['text'], truncation=True, padding=True)\n",
    "    return {'input_ids': enc['input_ids'], 'attention_mask': enc['attention_mask']}\n",
    "\n",
    "tokenized_filtered = filtered_data.map(_tokenize, remove_columns=filtered_data.column_names, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bca8e5b3-6117-4118-a228-5bd702a20eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 69813\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted = tokenized_filtered.train_test_split(test_size=0.01, seed=123)\n",
    "splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10205e65-8992-4af3-aa28-c8fad002db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f51a4a0-7803-4f04-8596-c40c788a743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(bbu, mlm=True)\n",
    "loader = DataLoader(dataset=splitted['train'], collate_fn=collator, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "becf73ca-6780-4600-b55e-0738014219be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699\n"
     ]
    }
   ],
   "source": [
    "n_batches = 0\n",
    "for batch in loader:\n",
    "    n_batches += 1\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26013cee-83af-440a-bf7e-d544fb7cd5cc",
   "metadata": {},
   "source": [
    "8. Which among the following statements is true?\n",
    "\n",
    "- [ ] ReLU activation is used in DistilBert\n",
    "- [x] The default configuration has max_position_embedding is 512\n",
    "- [x] The default configuration has 6 transformer layers\n",
    "- [ ] Dropout is not used in self-attention layers of DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "957bac06-a67c-41aa-be4c-49a490f9687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "123666d8-b2c9-498e-b380-42749cf3e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_model = DistilBertForMaskedLM(DistilBertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de814a33-6d16-40d3-a8d1-9e519f668ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66985530\n"
     ]
    }
   ],
   "source": [
    "# 9. What is the total number of parameters of the default configuration of\n",
    "# DistilBert? (Write you answer in Millions)\n",
    "n_params = 0\n",
    "for p in db_model.parameters():\n",
    "    n_params += p.numel()\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8ebc6481-2dab-426c-b734-34e5268af265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.98553"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffdd92bc-3ce1-4ea5-98ac-bbc88641500b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DistilBertConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e738d-72f0-46f7-966c-1e67b46cc07f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "10. Consider rest of the configurations to remain constant, and make three variations of DistilBertConfig as described below:\n",
    "\n",
    "    a. Config1 : max_position_embedding = 256\n",
    "    \n",
    "    b. Config2 : max_position_embedding = 512\n",
    "    \n",
    "    c. Config3 : max_position_embedding = 1024\n",
    "\n",
    "\n",
    "Let the total number of parameters in each configuration be equal to Param1, Param2 and Param3 respectively.\n",
    "\n",
    "Which of the following statements is true?\n",
    "- [ ] Param1 = Param2 = Param3\n",
    "- [ ] Param1 > Param2 > Param3\n",
    "- [x] Param1 < Param2 < Param3\n",
    "- [ ] absolute(Param1 - Param2) > absolute (Param2 - Param3)\n",
    "- [x] absolute(Param1 - Param2) < absolute (Param2 - Param3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2256c097-ff1c-4e31-90cf-cc6b1979472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 66788922\n",
      "2 66985530\n",
      "3 67378746\n"
     ]
    }
   ],
   "source": [
    "cfg1 = DistilBertConfig(max_position_embeddings=256)\n",
    "cfg2 = DistilBertConfig(max_position_embeddings=512)\n",
    "cfg3 = DistilBertConfig(max_position_embeddings=1024)\n",
    "\n",
    "for i, config in enumerate([cfg1, cfg2, cfg3], start=1):\n",
    "    _model = DistilBertForMaskedLM(config)\n",
    "    n_params = 0\n",
    "    for p in _model.parameters():\n",
    "        n_params += p.numel()\n",
    "    print(i, n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1143e150-a8e6-4890-b9f1-4fd2eca59e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = 66788922\n",
    "param2 = 66985530\n",
    "param3 = 67378746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bedce923-14bd-46a5-a40c-5fd081a6ff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(param1 - param2) < abs(param2 - param3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e938910c-7804-4cfd-95d2-9f6e08fe74f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "11. How many dimensions does the tensor output by the base Transformer model have, and what are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5b8c46ec-c352-4305-86fc-45eac35c6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "afe92880-7fab-4229-9e9f-e42ad1a0d47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDistilBertForMaskedLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "DistilBert Model with a `masked language modeling` head on top.\n",
       "\n",
       "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
       "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
       "etc.)\n",
       "\n",
       "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
       "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
       "and behavior.\n",
       "\n",
       "Parameters:\n",
       "    config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.\n",
       "        Initializing with a config file does not load the weights associated with the model, only the\n",
       "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/envs/dlp/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DistilBertForMaskedLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80876b51-485e-4196-a572-d484880ea0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
