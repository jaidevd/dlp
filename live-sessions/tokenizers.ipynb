{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab41a921-8777-466e-b63b-f752f5eb2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2d38a60c054b11b47bb1cf263bb4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d932bd26729849c1a7279a59dfe43a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cac32d3bc445b783a2b2bcc1ca22c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e2236e0f6745b6b285c8d069cabb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79e1e62c3b24cc9bd101bfffb359116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0ea30736f24fe8af734824d1b9960a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c78a2f432c4335895d2d44fc600374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0060508a-6d05-4e51-a5d9-c823038f14e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68cd384d-95fa-408a-a817-84a8aa0a6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(ds, batch_size):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i:(i + batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e6b67d-5bfc-499a-8e7a-010dde7fc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tok\n",
    "\n",
    "tokenizer = tok.Tokenizer(tok.models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = tok.normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b820f5c0-86cd-4003-b0db-c95574010e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or a sequence of normalizers!\n",
    "tokenizer.normalizer = tok.normalizers.Sequence([\n",
    "    tok.normalizers.NFD(),\n",
    "    tok.normalizers.Lowercase(),\n",
    "    tok.normalizers.StripAccents()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edb7540f-f88d-4295-aed0-39249fdd477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Allows concatenating multiple other Normalizer as a Sequence.\n",
       "All the normalizers run in sequence in the given order\n",
       "\n",
       "Args:\n",
       "    normalizers (:obj:`List[Normalizer]`):\n",
       "        A list of Normalizer to be run as a sequence\n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/envs/dlp/lib/python3.12/site-packages/tokenizers/normalizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok.normalizers.Sequence??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82a4eda3-0fbd-47c9-bb39-6c8962544deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('is', (5, 7)),\n",
       " ('an', (8, 10)),\n",
       " ('example', (11, 18)),\n",
       " ('!', (18, 19))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = tok.pre_tokenizers.BertPreTokenizer()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str('This is an example!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d908c912-5b0b-4e50-85c6-5e13fa46a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe91e188-9597-4e5b-b91e-7edc6e073c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaidevd/conda/envs/dlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135c134a-f18f-444a-80e3-673fff5e5ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c42f6-4158-463f-a1a7-b0988137be10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
